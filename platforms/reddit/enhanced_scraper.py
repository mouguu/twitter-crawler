#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆUofT Redditçˆ¬è™«
æ”¯æŒå¤§è§„æ¨¡çˆ¬å–ã€çŠ¶æ€è®°å½•ã€å»é‡åŠŸèƒ½
"""

import requests
import json
import time
import re
import os
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from local_storage import local_data_manager
import sys

# æ·»åŠ PRAWæ”¯æŒ
try:
    import praw
    PRAW_AVAILABLE = True
    print("ğŸš€ PRAWå·²å®‰è£…ï¼Œå°†ä½¿ç”¨Redditå®˜æ–¹API")
except ImportError:
    PRAW_AVAILABLE = False
    print("âš ï¸ PRAWæœªå®‰è£…ï¼Œä½¿ç”¨ä¼ ç»ŸJSON API")

class SmartRateController:
    """æ™ºèƒ½é€Ÿç‡æ§åˆ¶å™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰åçˆ¬ç­–ç•¥"""
    def __init__(self):
        self.base_delay = 1.0
        self.current_delay = 1.0
        self.consecutive_429s = 0
        self.success_streak = 0
        self.last_429_time = 0
        self.total_requests = 0
        self.successful_requests = 0

        # å†·å´æ¨¡å¼ç›¸å…³
        self.cooldown_mode = False
        self.cooldown_start_time = 0
        self.session_refresh_needed = False

    def record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        self.total_requests += 1
        self.successful_requests += 1
        self.success_streak += 1
        self.consecutive_429s = 0

        # å¦‚æœåœ¨å†·å´æ¨¡å¼ä¸­æˆåŠŸï¼Œå¯èƒ½å¯ä»¥é€€å‡ºå†·å´æ¨¡å¼
        if self.cooldown_mode and self.success_streak >= 3:
            self.exit_cooldown_mode()

        # è¿ç»­æˆåŠŸæ—¶é€æ¸å‡å°‘å»¶è¿Ÿ
        if self.success_streak > 10 and self.current_delay > 0.5:
            self.current_delay *= 0.95

    def record_429_error(self):
        """è®°å½•429é™æµé”™è¯¯"""
        self.total_requests += 1
        self.consecutive_429s += 1
        self.success_streak = 0
        self.last_429_time = time.time()

        # æŒ‡æ•°é€€é¿ç­–ç•¥
        if self.consecutive_429s <= 3:
            self.current_delay *= 2.0
        else:
            self.current_delay *= 1.5

        # é™åˆ¶æœ€å¤§å»¶è¿Ÿ
        self.current_delay = min(30.0, self.current_delay)

        # è§¦å‘å†·å´æ¨¡å¼å’Œä¼šè¯åˆ·æ–°
        if self.consecutive_429s >= 2:  # è¿ç»­2æ¬¡429å°±è¿›å…¥å†·å´æ¨¡å¼
            self.enter_cooldown_mode()

    def record_other_error(self):
        """è®°å½•å…¶ä»–é”™è¯¯"""
        self.total_requests += 1
        self.success_streak = 0
        # è½»å¾®å¢åŠ å»¶è¿Ÿ
        self.current_delay *= 1.1

    def get_delay(self):
        """è·å–å½“å‰åº”è¯¥ä½¿ç”¨çš„å»¶è¿Ÿ"""
        # å¦‚æœæœ€è¿‘é‡åˆ°429ï¼Œé¢å¤–å¢åŠ å»¶è¿Ÿ
        if time.time() - self.last_429_time < 60:
            return self.current_delay * 2
        return max(0.5, self.current_delay)

    def should_skip_strategy(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡å½“å‰ç­–ç•¥"""
        return self.consecutive_429s >= 5

    def get_success_rate(self):
        """è·å–æˆåŠŸç‡"""
        if self.total_requests == 0:
            return 1.0
        return self.successful_requests / self.total_requests

    def enter_cooldown_mode(self):
        """è¿›å…¥å†·å´æ¨¡å¼ - å¯ç”¨æ›´æ¿€è¿›çš„ååˆ¶æªæ–½"""
        if not self.cooldown_mode:
            self.cooldown_mode = True
            self.cooldown_start_time = time.time()
            self.session_refresh_needed = True
            print(f"ğŸ§Š è¿›å…¥å†·å´æ¨¡å¼ (è¿ç»­{self.consecutive_429s}æ¬¡429é”™è¯¯)")

    def exit_cooldown_mode(self):
        """é€€å‡ºå†·å´æ¨¡å¼"""
        if self.cooldown_mode:
            self.cooldown_mode = False
            self.session_refresh_needed = False
            print(f"ğŸŒ¡ï¸ é€€å‡ºå†·å´æ¨¡å¼ (è¿ç»­{self.success_streak}æ¬¡æˆåŠŸ)")

    def needs_session_refresh(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ä¼šè¯"""
        return self.session_refresh_needed

    def mark_session_refreshed(self):
        """æ ‡è®°ä¼šè¯å·²åˆ·æ–°"""
        self.session_refresh_needed = False

    def get_cooldown_wait_time(self):
        """è·å–å†·å´æ¨¡å¼ä¸‹çš„ç­‰å¾…æ—¶é—´"""
        if not self.cooldown_mode:
            return 0

        # æ ¹æ®è¿ç»­429é”™è¯¯æ•°é‡å†³å®šç­‰å¾…æ—¶é—´
        if self.consecutive_429s <= 3:
            return random.uniform(10, 20)
        elif self.consecutive_429s <= 5:
            return random.uniform(20, 40)
        else:
            return random.uniform(40, 60)

class EnhancedUofTScraper:
    def __init__(self, target_subreddit="UofT"):
        self.main_subreddit = target_subreddit
        # Reddit APIé…ç½®
        self.reddit_api = None
        if PRAW_AVAILABLE:
            try:
                self.reddit_api = praw.Reddit(
                    client_id="Oe2HbHnaZ_j7guwvxKTL2w",
                    client_secret="nV2EotsgBr0H3pTABCuPkNoMBSqedQ",
                    user_agent="UofT_Enhanced_Scraper_v2.0 by /u/YourUsername"
                )
                print("âœ… Redditå®˜æ–¹APIå·²è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ Reddit APIè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {e}")
                self.reddit_api = None

        # å¤šç»´åº¦çªç ´ç­–ç•¥é…ç½®
        self.breakthrough_strategies = {
            'time_dimensions': ['hour', 'day', 'week', 'month', 'year', 'all'],
            'sort_methods': ['hot', 'new', 'rising', 'best', 'controversial', 'top'],
            'special_sorts': ['gilded', 'promoted'],  # å¦‚æœæ”¯æŒ
            'search_operators': ['AND', 'OR', 'NOT', 'site:', 'author:', 'flair:'],
            'time_ranges': ['1h', '6h', '12h', '24h', '3d', '7d', '30d', '90d', '365d']
        }

        # User-Agentè½®æ¢æ±  - æ‰©å±•æ›´å¤šçœŸå®æµè§ˆå™¨
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]

        self.headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

        # æ™ºèƒ½é€Ÿç‡æ§åˆ¶å™¨ - ç»Ÿä¸€çš„é€Ÿç‡æ§åˆ¶æ¥æº
        self.rate_controller = SmartRateController()

        # å¤šç»´åº¦æ•°æ®æºé…ç½®
        # å¤šç»´åº¦æ•°æ®æºé…ç½®
        self.target_subreddits = [
            self.main_subreddit,       # ä¸»è¦ç›®æ ‡
        ]

        # æ‰©å±•å…³é”®è¯åº“ - å¤šç»´åº¦çªç ´
        self.extended_keywords = {
            'academic': [
                'course', 'exam', 'grade', 'professor', 'class', 'assignment', 'midterm', 'final',
                'mat137', 'mat135', 'mat136', 'csc148', 'csc165', 'csc236', 'csc207', 'csc209',
                'sta247', 'sta220', 'sta237', 'eco101', 'eco102', 'eco200', 'eco206',
                'phy131', 'phy132', 'che110', 'bio120', 'bio130', 'psy100', 'soc100', 'his103',
                'eng100', 'mat223', 'mat224', 'csc263', 'csc373', 'ece244', 'ece297'
            ],
            'campus_life': [
                'residence', 'dorm', 'housing', 'roommate', 'meal plan', 'dining hall', 'cafeteria',
                'robarts', 'gerstein', 'bahen', 'con hall', 'hart house', 'sid smith', 'medical sciences',
                'trinity', 'victoria', 'innis', 'woodsworth', 'new college', 'university college',
                'st george', 'utm', 'utsc', 'mississauga', 'scarborough'
            ],
            'admin_services': [
                'admission', 'application', 'waitlist', 'acceptance', 'enrollment', 'registration',
                'scholarship', 'osap', 'tuition', 'financial aid', 'bursary', 'fees',
                'acorn', 'quercus', 'degree explorer', 'transcript', 'gpa', 'cgpa'
            ],
            'career_future': [
                'internship', 'co-op', 'job', 'career', 'interview', 'resume', 'cv',
                'pey', 'work study', 'research opportunity', 'grad school', 'graduate school',
                'masters', 'phd', 'thesis', 'supervisor', 'lab', 'research'
            ],
            'social_events': [
                'frosh', 'orientation', 'convocation', 'graduation', 'clubs', 'societies',
                'events', 'parties', 'social', 'friends', 'dating', 'relationships'
            ]
        }



        # çŠ¶æ€è®°å½•
        self.scraped_count = 0
        self.skipped_count = 0
        self.error_count = 0

        # ä¸å†é¢„åŠ è½½æ‰€æœ‰IDï¼Œæ”¹ä¸ºæŒ‰éœ€æ‰¹é‡æ£€æŸ¥
        # self.existing_post_ids = set()  # ç§»é™¤è¿™ä¸ªå†…å­˜æ€æ‰‹

        # è·å–æ•°æ®åº“å¸–å­æ€»æ•°ï¼ˆä¸åŠ è½½æ‰€æœ‰IDï¼‰
        total_posts = self.get_database_post_count()
        print(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰ {total_posts} ä¸ªå¸–å­")

    def refresh_session(self):
        """åˆ·æ–°ä¼šè¯å’ŒUser-Agent"""
        self.session.close()
        self.session = requests.Session()
        self.headers['User-Agent'] = random.choice(self.user_agents)

        # æ·»åŠ æ›´å¤šéšæœºheadersæ¥æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.headers.update({
            'Accept-Language': random.choice(['en-US,en;q=0.9', 'en-GB,en;q=0.8', 'en-CA,en;q=0.7']),
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': random.choice(['max-age=0', 'no-cache']),
            'Sec-Ch-Ua': random.choice([
                '"Google Chrome";v="120", "Chromium";v="120", "Not_A Brand";v="24"',
                '"Microsoft Edge";v="120", "Chromium";v="120", "Not_A Brand";v="24"'
            ]),
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': random.choice(['"Windows"', '"macOS"', '"Linux"']),
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none'
        })

        self.session.headers.update(self.headers)
        print(f"ğŸ”„ å·²åˆ·æ–°ä¼šè¯å’ŒUser-Agent (æ›´çœŸå®çš„æµè§ˆå™¨æ¨¡æ‹Ÿ)")

        # æ ‡è®°ä¼šè¯å·²åˆ·æ–°
        if hasattr(self, 'rate_controller'):
            self.rate_controller.mark_session_refreshed()

    def simulate_human_behavior(self):
        """ç®€å•æ¨¡æ‹Ÿäººç±»è¡Œä¸º - éšæœºç­‰å¾…"""
        wait_time = random.uniform(3, 8)
        print(f"ğŸ­ æ¨¡æ‹Ÿäººç±»é˜…è¯»è¡Œä¸ºï¼Œç­‰å¾… {wait_time:.1f}s")
        time.sleep(wait_time)

    def handle_rate_limit_intelligently(self):
        """æ™ºèƒ½å¤„ç†429é™æµ - ç»Ÿä¸€çš„ååˆ¶ç­–ç•¥"""
        print(f"â³ é‡åˆ°é™æµ (è¿ç»­{self.rate_controller.consecutive_429s}æ¬¡)")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ä¼šè¯
        if self.rate_controller.needs_session_refresh():
            print("ğŸ”„ åˆ·æ–°ä¼šè¯ä»¥è§„é¿æ£€æµ‹...")
            self.refresh_session()

        # å¦‚æœåœ¨å†·å´æ¨¡å¼ï¼Œä½¿ç”¨æ›´æ¿€è¿›çš„ç­–ç•¥
        if self.rate_controller.cooldown_mode:
            print("ğŸ§Š å†·å´æ¨¡å¼æ¿€æ´»ï¼Œä½¿ç”¨æ¿€è¿›ååˆ¶ç­–ç•¥...")

            # ç­–ç•¥1: æ¨¡æ‹Ÿäººç±»è¡Œä¸º
            self.simulate_human_behavior()

            # ç­–ç•¥2: å†·å´ç­‰å¾…
            cooldown_time = self.rate_controller.get_cooldown_wait_time()
            print(f"â„ï¸ å†·å´ç­‰å¾… {cooldown_time:.1f}s...")
            time.sleep(cooldown_time)
        else:
            # å¸¸è§„å»¶è¿Ÿ
            delay = self.rate_controller.get_delay()
            print(f"â±ï¸ å¸¸è§„å»¶è¿Ÿ {delay:.1f}s...")
            time.sleep(delay)

    def check_posts_exist_batch(self, post_ids: list) -> set:
        """æ‰¹é‡æ£€æŸ¥å¸–å­æ˜¯å¦å·²å­˜åœ¨ï¼ˆå†…å­˜é«˜æ•ˆç‰ˆæœ¬ï¼Œçªç ´1000æ¡é™åˆ¶ï¼‰"""
        if not post_ids:
            return set()

        try:
            # Local storage check
            existing_ids = set()
            for post_id in post_ids:
                if local_data_manager.check_post_exists(post_id):
                    existing_ids.add(post_id)
            return existing_ids

        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡æ£€æŸ¥å¸–å­å­˜åœ¨æ€§å¤±è´¥: {e}")
            return set()

    def get_database_post_count(self):
        """è·å–æœ¬åœ°å·²ä¿å­˜å¸–å­æ€»æ•°"""
        return local_data_manager.get_posts_count()

    def sanitize_filename(self, text, max_length=50):
        """æ¸…ç†æ–‡ä»¶å"""
        text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
        text = re.sub(r'[<>:"/\\|?*]', '', text)
        text = re.sub(r'\s+', '_', text.strip())
        if len(text) > max_length:
            text = text[:max_length]
        return text

    def create_output_directory(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(script_dir))
        
        # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ output/reddit
        data_dir = os.path.join(project_root, 'output', 'reddit')
        
        # ç¡®ä¿Dataç›®å½•å­˜åœ¨
        os.makedirs(data_dir, exist_ok=True)

        # åœ¨Dataç›®å½•ä¸‹åˆ›å»ºå¸¦æ—¥æœŸçš„å­ç›®å½•
        today = datetime.now().strftime("%Y-%m-%d")
        base_dir = os.path.join(data_dir, f"scraped_{today}")

        counter = 1
        while os.path.exists(f"{base_dir}_{counter:03d}"):
            counter += 1

        output_dir = f"{base_dir}_{counter:03d}"
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        return output_dir

    def get_all_posts_paginated(self, max_posts=1000, sort_type='hot', time_filter='all', progress_callback=None):
        """åˆ†é¡µè·å–å¤§é‡å¸–å­"""
        post_urls = []
        after = None
        page = 1

        # print(f"ğŸ”„ å¼€å§‹è·å– {sort_type} æ¨¡å¼ä¸‹çš„å¸–å­ (æœ€å¤š {max_posts} ä¸ª)...")  # ç§»åˆ°è°ƒç”¨å¤„

        while len(post_urls) < max_posts:
            # æ„å»ºAPI URLï¼Œæ·»åŠ æ—¶é—´è¿‡æ»¤å™¨
            api_url = f"https://www.reddit.com/r/{self.main_subreddit}/{sort_type}.json?limit=100"
            if sort_type == 'top' and time_filter:
                api_url += f"&t={time_filter}"
            if after:
                api_url += f"&after={after}"

            try:
                print(f"ğŸ“„ è·å–ç¬¬ {page} é¡µ...", end=" ", flush=True)
                # ä½¿ç”¨æ™ºèƒ½é€Ÿç‡æ§åˆ¶ï¼Œé¡µæ•°è¶Šå¤šå»¶è¿Ÿè¶Šé•¿
                base_delay = self.rate_controller.get_delay()
                time.sleep(base_delay + (page % 3) * 0.5)  # é€’å¢å»¶è¿Ÿ

                response = self.session.get(api_url, timeout=30)

                if response.status_code == 403:
                    print("âŒ è¢«é˜»æ­¢ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
                    return self.get_posts_backup(max_posts)
                elif response.status_code == 429:
                    # ä½¿ç”¨æ™ºèƒ½é€Ÿç‡æ§åˆ¶å™¨å¤„ç†429é”™è¯¯
                    self.rate_controller.record_429_error()
                    self.handle_rate_limit_intelligently()
                    continue

                # è®°å½•æˆåŠŸè¯·æ±‚
                self.rate_controller.record_success()
                response.raise_for_status()
                data = response.json()
                posts = data['data']['children']

                if not posts:
                    print("âœ… å·²è·å–æ‰€æœ‰å¯ç”¨å¸–å­")
                    break

                new_posts = 0
                for post in posts:
                    if len(post_urls) >= max_posts:
                        break

                    post_data = post['data']
                    post_id = post_data['id']

                    # ç›´æ¥æ”¶é›†æ‰€æœ‰å¸–å­ï¼Œå»é‡åœ¨åç»­æ‰¹é‡å¤„ç†
                    post_url = f"https://www.reddit.com{post_data['permalink']}"
                    post_urls.append((post_url, post_id))
                    new_posts += 1
                
                if progress_callback:
                    try:
                        progress_callback(0, self.target_posts, f"Gathering candidates ({sort_type}): {len(post_urls)} found...")
                    except:
                        pass

                print(f"æ–°å¢ {new_posts} ä¸ªï¼Œè·³è¿‡ {len(posts) - new_posts} ä¸ªé‡å¤")

                # è·å–ä¸‹ä¸€é¡µå‚æ•°
                after = data['data']['after']
                if not after:
                    print("âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break

                page += 1

            except Exception as e:
                print(f"âŒ ç¬¬ {page} é¡µè·å–å¤±è´¥: {e}")
                break

        print(f"ğŸ“Š æ€»å…±è·å–åˆ° {len(post_urls)} ä¸ªæ–°å¸–å­URL")
        return post_urls

    def get_posts_backup(self, max_posts):
        """å¤‡ç”¨è·å–æ–¹æ¡ˆ"""
        backup_urls = [
            f"https://www.reddit.com/r/{self.main_subreddit}/.json?limit=100",
            f"https://old.reddit.com/r/{self.main_subreddit}/hot.json?limit=100"
        ]

        for backup_url in backup_urls:
            try:
                print(f"ğŸ”„ å°è¯•å¤‡ç”¨URL...")
                time.sleep(3)

                response = self.session.get(backup_url, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    posts = data['data']['children']

                    post_urls = []
                    for post in posts[:max_posts]:
                        post_data = post['data']
                        post_id = post_data['id']

                        # ç›´æ¥æ”¶é›†æ‰€æœ‰å¸–å­ï¼Œå»é‡åœ¨åç»­æ‰¹é‡å¤„ç†
                        post_url = f"https://www.reddit.com{post_data['permalink']}"
                        post_urls.append((post_url, post_id))

                    if post_urls:
                        print(f"âœ… å¤‡ç”¨æ–¹æ¡ˆæˆåŠŸï¼Œè·å–åˆ° {len(post_urls)} ä¸ªå¸–å­")
                        return post_urls

            except Exception as e:
                print(f"âœ— å¤‡ç”¨URLå¤±è´¥: {e}")
                continue

        return []

    def filter_new_posts_batch(self, post_urls_with_ids, batch_size=100):
        """æ‰¹é‡è¿‡æ»¤æ–°å¸–å­ï¼ˆå†…å­˜é«˜æ•ˆç‰ˆæœ¬ï¼‰"""
        if not post_urls_with_ids:
            return []

        new_posts = []

        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å•æ¬¡æŸ¥è¯¢è¿‡å¤§
        for i in range(0, len(post_urls_with_ids), batch_size):
            batch = post_urls_with_ids[i:i + batch_size]
            post_ids = [post_id for _, post_id in batch]

            # æ‰¹é‡æ£€æŸ¥è¿™äº›IDæ˜¯å¦å­˜åœ¨
            existing_ids = self.check_posts_exist_batch(post_ids)

            # è¿‡æ»¤å‡ºæ–°çš„å¸–å­
            for post_url, post_id in batch:
                if post_id not in existing_ids:
                    new_posts.append((post_url, post_id))

        return new_posts

    def get_recent_posts_multi_strategy(self, max_posts=5000, strategy_type="super_full", progress_callback=None):
        """å¤šç­–ç•¥è·å–å¤§é‡æœ€æ–°å¸–å­ - çªç ´APIé™åˆ¶"""
        print(f"ğŸš€ å¯åŠ¨è¶…çº§å¤šç­–ç•¥è·å– (ç›®æ ‡: {max_posts} ä¸ª)")
        print(f"ğŸ“‹ ç­–ç•¥ç±»å‹: {strategy_type}")
        print("=" * 50)
        all_post_urls = []

        # æ ¹æ®ç­–ç•¥ç±»å‹è°ƒæ•´æ‰§è¡Œé¡ºåºå’Œé‡ç‚¹
        if strategy_type == "super_recent":
            # æ—¶æ•ˆä¼˜å…ˆç­–ç•¥ - ä¼˜åŒ–æ‰§è¡Œé¡ºåºï¼Œæœ€æœ‰æ•ˆçš„ç­–ç•¥ä¼˜å…ˆ
            strategies = [
                ("æ·±åº¦å†å²æŒ–æ˜", None),      # æœ€æœ‰æ•ˆï¼Œä¼˜å…ˆæ‰§è¡Œ
                ("æ—¶é—´èŒƒå›´top", ['day', 'week', 'month', 'year']),
                ("æœ€æ–°æ’åº", ['new', 'rising']),
                ("å…³é”®è¯æœç´¢", None)          # æœ€åæ‰§è¡Œï¼Œå¸¦ä¿æŠ¤
            ]
        elif strategy_type == "super_popular":
            # çƒ­é—¨ä¼˜å…ˆç­–ç•¥
            strategies = [
                ("çƒ­é—¨æ’åº", ['hot', 'best']),
                ("æ—¶é—´èŒƒå›´top", ['all', 'year', 'month']),
                ("æ·±åº¦å†å²æŒ–æ˜", None),
                ("å…³é”®è¯æœç´¢", None)
            ]
        elif strategy_type == "super_search":
            # æœç´¢ä¼˜å…ˆç­–ç•¥
            strategies = [
                ("å…³é”®è¯æœç´¢", None),
                ("æ—¶é—´èŒƒå›´top", ['month', 'year', 'all']),
                ("çƒ­é—¨æ’åº", ['hot', 'new']),
                ("æ·±åº¦å†å²æŒ–æ˜", None)
            ]
        else:
            # å…¨é¢ç­–ç•¥ (é»˜è®¤) - å¤šç»´åº¦çªç ´å‡çº§ç‰ˆ
            strategies = [
                ("PRAWå¢å¼ºè·å–", None),        # æ–°å¢ï¼šä½¿ç”¨å®˜æ–¹API
                ("æ—¶é—´èŒƒå›´top", ['day', 'week', 'month', 'year', 'all']),
                ("å¤šç§æ’åº", ['hot', 'new', 'rising', 'best']),
                ("æ·±åº¦å†å²æŒ–æ˜", None),
                ("é«˜çº§æœç´¢ç­–ç•¥", None),        # æ–°å¢ï¼šé«˜çº§ç»„åˆæœç´¢
                ("å…³é”®è¯æœç´¢", None)
            ]

        # æ‰§è¡Œç­–ç•¥ - ä¿®å¤æ™ºèƒ½é¥±å’Œåº¦æ£€æµ‹é€»è¾‘
        strategy_results = []  # è®°å½•æ¯ä¸ªç­–ç•¥çš„æ•ˆæœ
        consecutive_low_gains = 0  # è¿ç»­ä½æ”¶ç›Šç­–ç•¥è®¡æ•°

        for strategy_name, params in strategies:
            # ä¿®æ”¹é€€å‡ºæ¡ä»¶ï¼šä¸ä»…è¦çœ‹å€™é€‰å¸–å­æ•°é‡ï¼Œè¿˜è¦è€ƒè™‘ç­–ç•¥å¤šæ ·æ€§
            # è‡³å°‘æ‰§è¡Œå‰3ä¸ªç­–ç•¥ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§æ¥æ‰¾åˆ°æ–°å¸–å­
            strategy_index = strategies.index((strategy_name, params))
            if len(all_post_urls) >= max_posts * 3 and strategy_index >= 2:
                print(f"ğŸ¯ å·²è·å–è¶³å¤Ÿå€™é€‰å¸–å­ ({len(all_post_urls)} ä¸ª) ä¸”æ‰§è¡Œäº†å¤šç§ç­–ç•¥ï¼Œåœæ­¢è·å–")
                break

            print(f"\nğŸ”„ æ‰§è¡Œç­–ç•¥: {strategy_name}")
            # ç¡®ä¿remainingä¸ä¼šæ˜¯è´Ÿæ•°ï¼Œè‡³å°‘ç»™æ¯ä¸ªç­–ç•¥ä¸€äº›æœç´¢é…é¢
            remaining = max(100, max_posts - len(all_post_urls))
            before_count = len(all_post_urls)

            if strategy_name == "PRAWå¢å¼ºè·å–":
                all_post_urls.extend(self._execute_praw_strategy(remaining, all_post_urls))
            elif strategy_name == "æ—¶é—´èŒƒå›´top":
                all_post_urls.extend(self._execute_time_range_strategy(params, remaining, all_post_urls))
            elif strategy_name in ["å¤šç§æ’åº", "æœ€æ–°æ’åº", "çƒ­é—¨æ’åº"]:
                all_post_urls.extend(self._execute_sort_strategy(params, remaining, all_post_urls))
            elif strategy_name == "é«˜çº§æœç´¢ç­–ç•¥":
                all_post_urls.extend(self._execute_advanced_search_strategy(remaining, all_post_urls))
            elif strategy_name == "å…³é”®è¯æœç´¢":
                all_post_urls.extend(self._execute_search_strategy(remaining, all_post_urls))
            elif strategy_name in ["æ·±åº¦åˆ†é¡µ", "æ·±åº¦å†å²æŒ–æ˜"]:
                all_post_urls.extend(self._execute_deep_paging_strategy(remaining, all_post_urls))

            # è®°å½•ç­–ç•¥æ•ˆæœ
            strategy_gain = len(all_post_urls) - before_count
            strategy_results.append(strategy_gain)

            print(f"ğŸ“Š å½“å‰ç­–ç•¥æ–°å¢: {strategy_gain} ä¸ªå¸–å­ï¼Œæ€»è®¡: {len(all_post_urls)} ä¸ª")
            
            if progress_callback:
                try:
                    progress_callback(0, self.target_posts, f"Gathering candidates ({strategy_name}): {len(all_post_urls)} found...")
                except:
                    pass

            # ä¿®å¤çš„æ™ºèƒ½é¥±å’Œåº¦æ£€æµ‹ - åªæœ‰åœ¨çœŸæ­£ä½æ”¶ç›Šæ—¶æ‰è§¦å‘
            if strategy_gain < 5:  # å•ä¸ªç­–ç•¥æ”¶ç›Šå¾ˆä½
                consecutive_low_gains += 1
            else:
                consecutive_low_gains = 0  # é‡ç½®è®¡æ•°å™¨

            # åªæœ‰è¿ç»­å¤šä¸ªç­–ç•¥éƒ½ä½æ”¶ç›Šæ—¶æ‰è€ƒè™‘æå‰ç»“æŸ
            if consecutive_low_gains >= 3 and len(strategy_results) >= 4:
                recent_total = sum(strategy_results[-3:])
                print(f"\nğŸ¯ æ£€æµ‹åˆ°æ•°æ®æºæ¥è¿‘é¥±å’Œ (æœ€è¿‘3ä¸ªç­–ç•¥ä»…æ–°å¢ {recent_total} ä¸ªå¸–å­)")

                # å¦‚æœå·²ç»è·å¾—äº†è¶³å¤Ÿçš„å€™é€‰å¸–å­ï¼ˆè‡³å°‘æ˜¯ç›®æ ‡çš„2å€ï¼‰ï¼Œå¯ä»¥æå‰ç»“æŸ
                if len(all_post_urls) >= max_posts * 2:
                    print(f"âœ… å·²è·å–è¶³å¤Ÿå€™é€‰å¸–å­ ({len(all_post_urls)} ä¸ª)ï¼Œæå‰ç»“æŸç­–ç•¥æ‰§è¡Œ")
                    break
                else:
                    print(f"ğŸ”„ ç»§ç»­æ‰§è¡Œå‰©ä½™ç­–ç•¥ä»¥è·å–æ›´å¤šå€™é€‰å¸–å­...")
                    consecutive_low_gains = 0  # é‡ç½®ï¼Œç»§ç»­å°è¯•

        print(f"\nğŸ‰ è¶…çº§å¤šç­–ç•¥è·å–å®Œæˆï¼æ€»å…±è·å¾— {len(all_post_urls)} ä¸ªæ–°å¸–å­URL")

        # å¦‚æœè·å¾—çš„å€™é€‰å¸–å­æ•°é‡è¿œè¶…ç›®æ ‡ï¼Œè¿”å›é€‚é‡çš„å€™é€‰å¸–å­
        # è¿™æ ·å¯ä»¥æé«˜æ‰¾åˆ°æ–°å¸–å­çš„æ¦‚ç‡
        if len(all_post_urls) > max_posts * 2:
            return all_post_urls[:max_posts * 2]
        else:
            return all_post_urls

    def _execute_time_range_strategy(self, time_ranges, max_posts, existing_urls):
        """æ‰§è¡Œæ—¶é—´èŒƒå›´ç­–ç•¥"""
        new_urls = []
        for time_range in time_ranges:
            if len(new_urls) >= max_posts:
                break

            print(f"  ğŸ“… è·å– top({time_range}) å¸–å­...")
            remaining = max(100, max_posts - len(new_urls))  # ç¡®ä¿è‡³å°‘æœç´¢100ä¸ª
            posts = self.get_all_posts_paginated(min(remaining, 1000), 'top', time_range)

            # æ”¶é›†æ‰€æœ‰å¸–å­ï¼Œåªè¿›è¡Œæœ¬è½®å†…å»é‡
            batch_new = []
            collected_ids = {post_id for _, post_id in existing_urls + new_urls}

            for post_url, post_id in posts:
                if post_id not in collected_ids:
                    batch_new.append((post_url, post_id))
                    collected_ids.add(post_id)

            new_urls.extend(batch_new)
            print(f"    âœ… top({time_range}) æ–°å¢ {len(batch_new)} ä¸ªå¸–å­")

        return new_urls

    def _execute_sort_strategy(self, sort_types, max_posts, existing_urls):
        """æ‰§è¡Œæ’åºç­–ç•¥"""
        new_urls = []
        for sort_type in sort_types:
            if len(new_urls) >= max_posts:
                break

            print(f"  ğŸ”„ è·å– {sort_type} å¸–å­...")
            remaining = max(100, max_posts - len(new_urls))  # ç¡®ä¿è‡³å°‘æœç´¢100ä¸ª
            posts = self.get_all_posts_paginated(min(remaining, 1000), sort_type)

            # æ”¶é›†æ‰€æœ‰å¸–å­ï¼Œåªè¿›è¡Œæœ¬è½®å†…å»é‡
            batch_new = []
            collected_ids = {post_id for _, post_id in existing_urls + new_urls}

            for post_url, post_id in posts:
                if post_id not in collected_ids:
                    batch_new.append((post_url, post_id))
                    collected_ids.add(post_id)

            new_urls.extend(batch_new)
            print(f"    âœ… {sort_type} æ–°å¢ {len(batch_new)} ä¸ªå¸–å­")

        return new_urls

    def _execute_search_strategy(self, max_posts, existing_urls):
        """æ‰§è¡Œæœç´¢ç­–ç•¥ - ä¿®å¤ç»Ÿè®¡é—®é¢˜"""
        print(f"  ğŸ” å…³é”®è¯æœç´¢è·å–å¸–å­...")
        search_posts = self.search_posts_by_keywords(max_posts)

        new_search_posts = []
        collected_ids = set()

        for post_url, post_id in search_posts:
            # åªæ£€æŸ¥æœ¬è½®æ”¶é›†çš„é‡å¤ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
            if (not any(existing_id == post_id for _, existing_id in existing_urls) and
                post_id not in collected_ids):
                new_search_posts.append((post_url, post_id))
                collected_ids.add(post_id)
            # ç§»é™¤duplicate_countç»Ÿè®¡ï¼Œå› ä¸ºçœŸæ­£çš„å»é‡åœ¨åé¢è¿›è¡Œ

        print(f"    âœ… å…³é”®è¯æœç´¢ æ‰¾åˆ° {len(search_posts)} ä¸ªï¼Œæ–°å¢ {len(new_search_posts)} ä¸ª")
        return new_search_posts

    def _execute_deep_paging_strategy(self, max_posts, existing_urls):
        """æ‰§è¡Œæ·±åº¦å†å²æŒ–æ˜ç­–ç•¥ - é¿å…é‡å¤å·²æ‰§è¡Œçš„ç­–ç•¥"""
        print(f"  ğŸ“š æ·±åº¦å†å²æŒ–æ˜è·å–å¸–å­...")
        deep_posts = self.get_deep_historical_posts(max_posts)

        new_deep_posts = []
        collected_ids = set()

        for post_url, post_id in deep_posts:
            # åªæ£€æŸ¥æœ¬è½®æ”¶é›†çš„é‡å¤ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
            if (not any(existing_id == post_id for _, existing_id in existing_urls) and
                post_id not in collected_ids):
                new_deep_posts.append((post_url, post_id))
                collected_ids.add(post_id)

        print(f"    âœ… æ·±åº¦å†å²æŒ–æ˜ æ‰¾åˆ° {len(deep_posts)} ä¸ªï¼Œæ–°å¢ {len(new_deep_posts)} ä¸ª")
        return new_deep_posts

    def _execute_praw_strategy(self, max_posts, existing_urls):
        """æ‰§è¡ŒPRAWå¢å¼ºç­–ç•¥ï¼ˆå¸¦è‡ªåŠ¨é™çº§ï¼‰"""
        print(f"  ğŸš€ PRAWå¢å¼ºå¤šç»´åº¦è·å–...")
        praw_posts = self.get_praw_enhanced_posts(max_posts)

        # å¦‚æœPRAWå¤±è´¥ï¼ˆè¿”å›ç©ºåˆ—è¡¨ï¼‰ï¼Œè‡ªåŠ¨é™çº§åˆ°JSON API
        if not praw_posts:
            print(f"  âš ï¸ PRAWè·å–å¤±è´¥ï¼Œè‡ªåŠ¨é™çº§åˆ°JSON APIå¤‡ç”¨æ–¹æ¡ˆ...")
            # ä½¿ç”¨JSON APIçš„å¤šç§æ’åºæ–¹å¼ä½œä¸ºå¤‡ç”¨
            backup_posts = []
            for sort_type in ['hot', 'new', 'top']:
                try:
                    print(f"    ğŸ”„ å¤‡ç”¨æ–¹æ¡ˆ: {sort_type} æ’åº...")
                    batch_posts = self.get_all_posts_paginated(min(max_posts//3, 500), sort_type)
                    backup_posts.extend(batch_posts)
                    if len(backup_posts) >= max_posts:
                        break
                except Exception as e:
                    print(f"    âœ— {sort_type} å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {e}")
                    continue
            praw_posts = backup_posts[:max_posts]
            if praw_posts:
                print(f"    âœ… å¤‡ç”¨æ–¹æ¡ˆæˆåŠŸè·å– {len(praw_posts)} ä¸ªå¸–å­")

        new_praw_posts = []
        collected_ids = set()

        for post_url, post_id in praw_posts:
            # åªæ£€æŸ¥æœ¬è½®æ”¶é›†çš„é‡å¤ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
            if (not any(existing_id == post_id for _, existing_id in existing_urls) and
                post_id not in collected_ids):
                new_praw_posts.append((post_url, post_id))
                collected_ids.add(post_id)

        print(f"    âœ… PRAWç­–ç•¥ æ‰¾åˆ° {len(praw_posts)} ä¸ªï¼Œæ–°å¢ {len(new_praw_posts)} ä¸ª")
        return new_praw_posts

    def _execute_advanced_search_strategy(self, max_posts, existing_urls):
        """æ‰§è¡Œé«˜çº§æœç´¢ç­–ç•¥"""
        print(f"  ğŸ” é«˜çº§æœç´¢ç­–ç•¥è·å–...")
        search_posts = self.get_advanced_search_posts(max_posts)

        new_search_posts = []
        collected_ids = set()

        for post_url, post_id in search_posts:
            # åªæ£€æŸ¥æœ¬è½®æ”¶é›†çš„é‡å¤ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
            if (not any(existing_id == post_id for _, existing_id in existing_urls) and
                post_id not in collected_ids):
                new_search_posts.append((post_url, post_id))
                collected_ids.add(post_id)

        print(f"    âœ… é«˜çº§æœç´¢ æ‰¾åˆ° {len(search_posts)} ä¸ªï¼Œæ–°å¢ {len(new_search_posts)} ä¸ª")
        return new_search_posts

    def search_posts_by_keywords(self, max_posts=2000):
        """é€šè¿‡å…³é”®è¯æœç´¢è·å–æ›´å¤šå¸–å­ - ä¼˜åŒ–å…³é”®è¯åº“"""
        # åˆ†ç±»å…³é”®è¯ï¼Œæé«˜æœç´¢æ•ˆç‡
        academic_keywords = [
            'course', 'exam', 'grade', 'professor', 'class', 'assignment', 'midterm', 'final',
            'mat137', 'mat135', 'csc148', 'csc165', 'csc236', 'sta247', 'sta220', 'eco101', 'eco102',
            'phy131', 'phy132', 'che110', 'bio120', 'psy100', 'soc100', 'his103'
        ]

        life_keywords = [
            'residence', 'dorm', 'housing', 'roommate', 'meal plan', 'dining hall',
            'robarts', 'gerstein', 'bahen', 'con hall', 'hart house', 'sid smith',
            'trinity', 'victoria', 'innis', 'woodsworth', 'new college', 'university college'
        ]

        admin_keywords = [
            'admission', 'application', 'waitlist', 'acceptance', 'enrollment',
            'scholarship', 'osap', 'tuition', 'financial aid', 'bursary',
            'acorn', 'quercus', 'degree explorer', 'transcript'
        ]

        career_keywords = [
            'internship', 'co-op', 'job', 'career', 'interview', 'resume',
            'pey', 'work study', 'research opportunity', 'grad school'
        ]

        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        keywords = academic_keywords + life_keywords + admin_keywords + career_keywords

        post_urls = []
        all_found_posts = set()  # ç”¨äºå…³é”®è¯é—´å»é‡
        # ç¡®ä¿æ¯ä¸ªå…³é”®è¯è‡³å°‘æœç´¢1ä¸ªå¸–å­ï¼Œå³ä½¿max_postsæ˜¯è´Ÿæ•°
        posts_per_keyword = max(1, max(100, max_posts) // len(keywords))
        total_found = 0
        total_duplicates = 0

        print(f"  ğŸ“Š å¼€å§‹æœç´¢ {len(keywords)} ä¸ªå…³é”®è¯...")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æœç´¢ç­–ç•¥
        if self.rate_controller.should_skip_strategy():
            print(f"  ğŸ›‘ é€Ÿç‡æ§åˆ¶å»ºè®®è·³è¿‡å…³é”®è¯æœç´¢ (è¿ç»­{self.rate_controller.consecutive_429s}æ¬¡429é”™è¯¯)")
            return []

        for i, keyword in enumerate(keywords, 1):
            if len(post_urls) >= max_posts:
                break

            # å¦‚æœé€Ÿç‡æ§åˆ¶å»ºè®®è·³è¿‡ï¼Œæå‰ç»“æŸ
            if self.rate_controller.should_skip_strategy():
                print(f"  ğŸ›‘ é€Ÿç‡æ§åˆ¶å»ºè®®è·³è¿‡å‰©ä½™ {len(keywords) - i + 1} ä¸ªå…³é”®è¯")
                break

            try:
                search_url = "https://www.reddit.com/r/UofT/search.json"
                params = {
                    'q': keyword,
                    'sort': 'relevance',  # æ”¹ä¸ºç›¸å…³æ€§æ’åºï¼Œè·å¾—æ›´å¥½çš„ç»“æœ
                    'limit': 100,
                    'restrict_sr': 1,
                    't': 'all'
                }

                if i % 10 == 0:  # æ¯10ä¸ªå…³é”®è¯æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print(f"  ğŸ“ˆ è¿›åº¦: {i}/{len(keywords)} å…³é”®è¯ï¼Œå·²æ‰¾åˆ° {len(post_urls)} ä¸ªæ–°å¸–å­")

                time.sleep(self.rate_controller.get_delay() * 2)  # å…³é”®è¯æœç´¢ä½¿ç”¨2å€å»¶è¿Ÿ
                response = self.session.get(search_url, params=params, timeout=30)

                if response.status_code == 200:
                    self.rate_controller.record_success()
                    data = response.json()
                    posts = data['data']['children']
                    total_found += len(posts)

                    keyword_new = 0
                    keyword_duplicates = 0

                    for post in posts:
                        if len(post_urls) >= max_posts:
                            break

                        post_data = post['data']
                        post_id = post_data['id']

                        # åªæ£€æŸ¥æœ¬è½®å…³é”®è¯é—´çš„é‡å¤ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
                        if post_id not in all_found_posts:
                            post_url = f"https://www.reddit.com{post_data['permalink']}"
                            post_urls.append((post_url, post_id))
                            all_found_posts.add(post_id)
                            keyword_new += 1
                        else:
                            keyword_duplicates += 1

                    total_duplicates += keyword_duplicates

                    if keyword_new > 0:
                        print(f"  ğŸ” {keyword}: +{keyword_new} æ–°å¸–å­")

                else:
                    if response.status_code == 429:
                        self.rate_controller.record_429_error()
                        print(f"  âœ— {keyword}: æœç´¢é™æµ (429)")
                    else:
                        self.rate_controller.record_other_error()
                        print(f"  âœ— {keyword}: æœç´¢å¤±è´¥ ({response.status_code})")

            except Exception as e:
                self.rate_controller.record_other_error()
                print(f"  âœ— {keyword}: æœç´¢å‡ºé”™ ({e})")
                continue

        print(f"  ğŸ“Š å…³é”®è¯æœç´¢å®Œæˆ: æ€»å…±æ‰¾åˆ° {total_found} ä¸ªå¸–å­ï¼Œæ–°å¢ {len(post_urls)} ä¸ªï¼Œè·³è¿‡ {total_duplicates} ä¸ªé‡å¤")
        return post_urls

    def get_advanced_search_posts(self, max_posts=3000):
        """é«˜çº§æœç´¢ç­–ç•¥ - å¤šç»´åº¦ç»„åˆæœç´¢"""
        if not self.reddit_api:
            print("âš ï¸ PRAWä¸å¯ç”¨ï¼Œè·³è¿‡é«˜çº§æœç´¢")
            return []

        print("ğŸ” å¯åŠ¨é«˜çº§å¤šç»´åº¦æœç´¢...")
        post_urls = []
        subreddit = self.reddit_api.subreddit('UofT')

        # é«˜çº§æœç´¢ç»„åˆ
        advanced_searches = [
            # å­¦æœ¯ç›¸å…³ç»„åˆæœç´¢
            "course AND (grade OR mark OR exam)",
            "professor AND (review OR rating OR recommend)",
            "assignment AND (help OR question OR due)",
            "midterm OR final OR test OR quiz",

            # è¯¾ç¨‹ä»£ç ç»„åˆ
            "MAT137 OR MAT135 OR MAT136",
            "CSC148 OR CSC165 OR CSC236 OR CSC207",
            "STA247 OR STA220 OR STA237",
            "ECO101 OR ECO102 OR ECO200",

            # æ ¡å›­ç”Ÿæ´»ç»„åˆ
            "residence OR dorm OR housing OR roommate",
            "robarts OR gerstein OR bahen OR library",
            "trinity OR victoria OR innis OR college",

            # ç”³è¯·å’Œè¡Œæ”¿
            "admission OR application OR waitlist",
            "scholarship OR osap OR financial",
            "acorn OR quercus OR registration",

            # èŒä¸šå‘å±•
            "internship OR co-op OR job OR career",
            "pey OR work OR research OR lab",
            "grad school OR graduate OR masters OR phd",

            # æŒ‰å¹´ä»½æœç´¢ (æ—¶é—´ç»´åº¦)
            "2024", "2023", "2022", "2021", "2020",

            # æŒ‰å­¦æœŸæœç´¢
            "fall 2024", "winter 2024", "summer 2024",
            "fall 2023", "winter 2023", "summer 2023",

            # ç‰¹æ®Šè¯é¢˜
            "covid OR pandemic OR online OR remote",
            "strike OR protest OR tuition increase",
            "mental health OR stress OR anxiety",
            "dating OR relationship OR social"
        ]

        for search_query in advanced_searches:
            if len(post_urls) >= max_posts:
                break

            try:
                print(f"  ğŸ” é«˜çº§æœç´¢: {search_query[:50]}...")

                # ä½¿ç”¨PRAWæœç´¢ï¼Œæ”¯æŒæ›´å¤æ‚çš„æŸ¥è¯¢
                search_results = subreddit.search(
                    search_query,
                    sort='relevance',
                    time_filter='all',
                    limit=1000
                )

                batch_new = 0
                for submission in search_results:
                    if len(post_urls) >= max_posts:
                        break

                    post_id = submission.id
                    # ç›´æ¥æ”¶é›†ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
                    post_url = f"https://www.reddit.com{submission.permalink}"
                    post_urls.append((post_url, post_id))
                    batch_new += 1

                if batch_new > 0:
                    print(f"    âœ… æ–°å¢ {batch_new} ä¸ªå¸–å­")

                time.sleep(self.rate_controller.get_delay())  # æ™ºèƒ½é€Ÿç‡æ§åˆ¶

            except Exception as e:
                print(f"    âœ— æœç´¢å¤±è´¥: {e}")
                continue

        print(f"ğŸ‰ é«˜çº§æœç´¢å®Œæˆï¼Œæ€»å…±è·å¾— {len(post_urls)} ä¸ªæ–°å¸–å­")
        return post_urls

    def get_deep_historical_posts(self, max_posts=1000):
        """æ·±åº¦å†å²æŒ–æ˜ - ä½¿ç”¨ä¸åŒçš„ç­–ç•¥ç»„åˆé¿å…é‡å¤"""
        post_urls = []

        # ä½¿ç”¨ä¸åŒçš„æ’åº+æ—¶é—´ç»„åˆï¼Œé¿å…é‡å¤å·²æ‰§è¡Œçš„ç­–ç•¥
        historical_configs = [
            ('hot', 'year'), ('hot', 'all'),
            ('best', 'year'), ('best', 'all'),
            ('controversial', 'month'), ('controversial', 'year'), ('controversial', 'all'),
            ('gilded', 'year'), ('gilded', 'all')  # å¦‚æœæ”¯æŒçš„è¯
        ]

        for sort_type, time_filter in historical_configs:
            if len(post_urls) >= max_posts:
                break

            print(f"  ğŸ›ï¸ å†å²æŒ–æ˜: {sort_type}({time_filter})")

            # æ·±åº¦åˆ†é¡µè·å–
            after = None
            page = 1
            max_pages = 20  # æ›´æ·±çš„åˆ†é¡µ

            while page <= max_pages and len(post_urls) < max_posts:
                try:
                    api_url = f"https://www.reddit.com/r/UofT/{sort_type}.json?limit=100&t={time_filter}"
                    if after:
                        api_url += f"&after={after}"

                    # ä½¿ç”¨æ™ºèƒ½é€Ÿç‡æ§åˆ¶
                    time.sleep(self.rate_controller.get_delay())
                    response = self.session.get(api_url, timeout=30)

                    if response.status_code == 200:
                        self.rate_controller.record_success()
                        data = response.json()
                        posts = data['data']['children']

                        if not posts:
                            break

                        page_new = 0
                        for post in posts:
                            if len(post_urls) >= max_posts:
                                break

                            post_data = post['data']
                            post_id = post_data['id']

                            # ç›´æ¥æ”¶é›†ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
                            post_url = f"https://www.reddit.com{post_data['permalink']}"
                            post_urls.append((post_url, post_id))
                            page_new += 1

                        if page_new > 0:
                            print(f"    ğŸ“„ ç¬¬{page}é¡µ: +{page_new} ä¸ªå¸–å­")

                        # è·å–ä¸‹ä¸€é¡µå‚æ•°
                        after = data['data']['after']
                        if not after:
                            break

                        page += 1
                    else:
                        if response.status_code == 429:
                            self.rate_controller.record_429_error()
                            print(f"    âš ï¸ é‡åˆ°é™æµï¼Œå»¶è¿Ÿ {self.rate_controller.get_delay():.1f}s")
                            time.sleep(self.rate_controller.get_delay())
                        elif response.status_code == 404:
                            print(f"    âš ï¸ {sort_type}æ’åºä¸æ”¯æŒï¼Œè·³è¿‡")
                            break
                        else:
                            self.rate_controller.record_other_error()
                            print(f"    âœ— ç¬¬{page}é¡µå¤±è´¥: {response.status_code}")
                            break

                except Exception as e:
                    print(f"    âœ— ç¬¬{page}é¡µå‡ºé”™: {e}")
                    break

        return post_urls

    def get_praw_enhanced_posts(self, max_posts=2000):
        """ä½¿ç”¨PRAW APIè·å–æ›´å¤šå¸–å­ - å¤šç»´åº¦çªç ´"""
        if not self.reddit_api:
            print("âš ï¸ PRAWä¸å¯ç”¨ï¼Œè·³è¿‡å¢å¼ºè·å–")
            return []

        print("ğŸš€ å¯åŠ¨PRAWå¢å¼ºå¤šç»´åº¦è·å–...")
        post_urls = []
        subreddit = self.reddit_api.subreddit('UofT')

        # 1. æ‰©å±•æ—¶é—´ç»´åº¦è·å–
        time_methods = [
            ('hot', None), ('new', None), ('rising', None), ('best', None),
            ('top', 'hour'), ('top', 'day'), ('top', 'week'),
            ('top', 'month'), ('top', 'year'), ('top', 'all'),
            ('controversial', 'day'), ('controversial', 'week'),
            ('controversial', 'month'), ('controversial', 'year'), ('controversial', 'all')
        ]

        for sort_method, time_filter in time_methods:
            # ä¿®æ”¹é€€å‡ºæ¡ä»¶ï¼šå…è®¸æ”¶é›†æ›´å¤šå€™é€‰å¸–å­ä»¥æé«˜æ‰¾åˆ°æ–°å¸–å­çš„æ¦‚ç‡
            # è‡³å°‘æ‰§è¡Œå‰å‡ ä¸ªé‡è¦çš„æ–¹æ³•
            method_index = time_methods.index((sort_method, time_filter))
            if len(post_urls) >= max_posts * 3 and method_index >= 4:
                print(f"  ğŸ¯ å·²æ”¶é›†è¶³å¤Ÿå€™é€‰å¸–å­ ({len(post_urls)} ä¸ª)ï¼Œåœæ­¢PRAWè·å–")
                break

            try:
                print(f"  ğŸ”„ PRAWè·å–: {sort_method}({time_filter if time_filter else 'default'})")

                if sort_method == 'hot':
                    submissions = subreddit.hot(limit=1000)
                elif sort_method == 'new':
                    submissions = subreddit.new(limit=1000)
                elif sort_method == 'rising':
                    submissions = subreddit.rising(limit=1000)
                elif sort_method == 'best':
                    submissions = subreddit.best(limit=1000)
                elif sort_method == 'top':
                    submissions = subreddit.top(time_filter=time_filter, limit=1000)
                elif sort_method == 'controversial':
                    submissions = subreddit.controversial(time_filter=time_filter, limit=1000)
                else:
                    continue

                batch_new = 0
                for submission in submissions:
                    # ç§»é™¤å•ä¸ªæ–¹æ³•å†…çš„æå‰é€€å‡ºï¼Œè®©æ¯ä¸ªæ–¹æ³•éƒ½èƒ½å®Œæ•´æ‰§è¡Œ
                    post_id = submission.id
                    # ç›´æ¥æ”¶é›†ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
                    post_url = f"https://www.reddit.com{submission.permalink}"
                    post_urls.append((post_url, post_id))
                    batch_new += 1

                print(f"    âœ… {sort_method}({time_filter if time_filter else 'default'}) æ–°å¢ {batch_new} ä¸ªå¸–å­")

            except Exception as e:
                print(f"    âœ— {sort_method}({time_filter if time_filter else 'default'}) å¤±è´¥: {e}")
                continue

        print(f"ğŸ‰ PRAWå¢å¼ºè·å–å®Œæˆï¼Œæ€»å…±è·å¾— {len(post_urls)} ä¸ªæ–°å¸–å­")
        return post_urls

    def get_deep_paginated_posts(self, max_posts=1000):
        """æ·±åº¦åˆ†é¡µè·å–å†å²å¸–å­ - çªç ´åˆ†é¡µé™åˆ¶"""
        post_urls = []

        # ä½¿ç”¨å¤šç§æ’åºæ–¹å¼è¿›è¡Œæ·±åº¦åˆ†é¡µ
        sort_configs = [
            ('top', 'month'), ('top', 'year'), ('top', 'all'),
            ('hot', None), ('new', None)
        ]

        for sort_type, time_filter in sort_configs:
            if len(post_urls) >= max_posts:
                break

            print(f"  ğŸ“š æ·±åº¦åˆ†é¡µ: {sort_type}({time_filter if time_filter else 'default'})")

            # å°è¯•è·å–æ›´å¤šé¡µé¢
            after = None
            page = 1
            max_pages = 15  # å¢åŠ é¡µé¢æ•°é‡

            while page <= max_pages and len(post_urls) < max_posts:
                try:
                    api_url = f"https://www.reddit.com/r/UofT/{sort_type}.json?limit=100"
                    if time_filter:
                        api_url += f"&t={time_filter}"
                    if after:
                        api_url += f"&after={after}"

                    # ä½¿ç”¨æ™ºèƒ½é€Ÿç‡æ§åˆ¶ï¼Œé¡µæ•°è¶Šå¤šå»¶è¿Ÿè¶Šé•¿
                    base_delay = self.rate_controller.get_delay()
                    time.sleep(base_delay + (page % 5) * 0.2)  # é€’å¢å»¶è¿Ÿ
                    response = self.session.get(api_url, timeout=30)

                    if response.status_code == 200:
                        data = response.json()
                        posts = data['data']['children']

                        if not posts:
                            break

                        page_new = 0
                        for post in posts:
                            if len(post_urls) >= max_posts:
                                break

                            post_data = post['data']
                            post_id = post_data['id']

                            # ç›´æ¥æ”¶é›†ï¼Œæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batch
                            post_url = f"https://www.reddit.com{post_data['permalink']}"
                            post_urls.append((post_url, post_id))
                            page_new += 1

                        print(f"    ğŸ“„ ç¬¬{page}é¡µ: +{page_new} ä¸ªå¸–å­")

                        # è·å–ä¸‹ä¸€é¡µå‚æ•°
                        after = data['data']['after']
                        if not after:
                            break

                        page += 1
                    else:
                        print(f"    âœ— ç¬¬{page}é¡µå¤±è´¥: {response.status_code}")
                        break

                except Exception as e:
                    print(f"    âœ— ç¬¬{page}é¡µå‡ºé”™: {e}")
                    break

        return post_urls

    def search_posts_by_timeframe(self, max_posts=1000):
        """ä½¿ç”¨Redditæœç´¢APIæŒ‰æ—¶é—´æ®µè·å–å¸–å­"""
        post_urls = []

        # æœç´¢ä¸åŒæ—¶é—´æ®µçš„å¸–å­
        import time
        from datetime import datetime, timedelta

        # æœç´¢æœ€è¿‘30å¤©çš„å¸–å­ï¼ŒæŒ‰å‘¨åˆ†æ®µ
        for weeks_ago in range(0, 12):  # æœ€è¿‘12å‘¨
            if len(post_urls) >= max_posts:
                break

            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_date = datetime.now() - timedelta(weeks=weeks_ago)
            start_date = end_date - timedelta(weeks=1)

            # è½¬æ¢ä¸ºUnixæ—¶é—´æˆ³
            end_timestamp = int(end_date.timestamp())
            start_timestamp = int(start_date.timestamp())

            try:
                # ä½¿ç”¨Redditæœç´¢API
                search_url = f"https://www.reddit.com/r/UofT/search.json"
                params = {
                    'q': 'subreddit:UofT',
                    'sort': 'new',
                    'limit': 100,
                    'restrict_sr': 1,
                    't': 'all'
                }

                print(f"ğŸ“… æœç´¢ {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')} çš„å¸–å­...")
                time.sleep(self.rate_controller.get_delay())

                response = self.session.get(search_url, params=params, timeout=30)

                if response.status_code == 200:
                    data = response.json()
                    posts = data['data']['children']

                    week_posts = 0
                    for post in posts:
                        if len(post_urls) >= max_posts:
                            break

                        post_data = post['data']
                        post_id = post_data['id']
                        created_utc = post_data['created_utc']

                        # æ£€æŸ¥æ—¶é—´èŒƒå›´ï¼ˆæ•°æ®åº“å»é‡äº¤ç»™filter_new_posts_batchï¼‰
                        if start_timestamp <= created_utc <= end_timestamp:
                            post_url = f"https://www.reddit.com{post_data['permalink']}"
                            post_urls.append((post_url, post_id))
                            week_posts += 1

                    print(f"  âœ… è¯¥å‘¨æ–°å¢ {week_posts} ä¸ªå¸–å­")
                else:
                    print(f"  âš ï¸ æœç´¢å¤±è´¥: {response.status_code}")

            except Exception as e:
                print(f"  âŒ æœç´¢å‡ºé”™: {e}")
                continue

        return post_urls

    def scrape_post_with_comments(self, post_url, post_id):
        """æŠ“å–å•ä¸ªå¸–å­åŠå…¶è¯„è®º"""
        try:
            # ä¸éœ€è¦å†æ¬¡æ£€æŸ¥é‡å¤ï¼Œå› ä¸ºURLè·å–é˜¶æ®µå·²ç»è¿‡æ»¤äº†
            json_url = post_url.rstrip('/') + '.json'
            response = self.session.get(json_url, timeout=30)

            # ä½¿ç”¨æ™ºèƒ½é€Ÿç‡æ§åˆ¶å™¨å¤„ç†429é™æµé”™è¯¯
            if response.status_code == 429:
                self.rate_controller.record_429_error()
                self.handle_rate_limit_intelligently()

                # é‡è¯•ä¸€æ¬¡
                response = self.session.get(json_url, timeout=30)
                if response.status_code == 429:
                    print("â­ï¸ ä»ç„¶è¢«é™æµï¼Œè·³è¿‡æ­¤å¸–å­ç»§ç»­ä¸‹ä¸€ä¸ª")
                    return None

            # è®°å½•æˆåŠŸè¯·æ±‚
            self.rate_controller.record_success()

            response.raise_for_status()

            data = response.json()

            # æå–å¸–å­ä¿¡æ¯
            post_info = data[0]['data']['children'][0]['data']

            # æå–è¯„è®º
            comments_data = data[1]['data']['children'] if len(data) > 1 else []
            comments = self.extract_comments(comments_data)

            post_data = {
                'id': post_info['id'],
                'title': post_info['title'],
                'author': post_info.get('author', '[deleted]'),
                'score': post_info.get('score', 0),
                'selftext': post_info.get('selftext', ''),
                'url': post_url,
                'created_utc': post_info.get('created_utc', 0),
                'num_comments': post_info.get('num_comments', 0),
                'comments': comments
            }

            return post_data

        except Exception as e:
            self.error_count += 1
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return None

    def extract_comments(self, comments_data):
        """é€’å½’æå–è¯„è®º"""
        comments = []

        for comment_item in comments_data:
            if comment_item['kind'] == 't1':  # è¯„è®ºç±»å‹
                comment_data = comment_item['data']

                if comment_data.get('body') and comment_data['body'] != '[deleted]':
                    comment = {
                        'id': comment_data['id'],
                        'author': comment_data.get('author', '[deleted]'),
                        'body': comment_data['body'],
                        'score': comment_data.get('score', 0),
                        'replies': []
                    }

                    # é€’å½’å¤„ç†å›å¤
                    if 'replies' in comment_data and comment_data['replies']:
                        if isinstance(comment_data['replies'], dict):
                            replies_data = comment_data['replies']['data']['children']
                            comment['replies'] = self.extract_comments(replies_data)

                    comments.append(comment)

        return comments

    def save_to_database(self, post_data, max_retries=3):
        """ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        for attempt in range(max_retries):
            try:
                # å‡†å¤‡æ•°æ®åº“è®°å½•
                db_record = {
                    'post_id': post_data['id'],
                    'title': post_data['title'],
                    'author': post_data['author'],
                    'score': post_data['score'],
                    'num_comments': post_data['num_comments'],
                    'created_utc': int(float(post_data['created_utc'])),  # å…ˆè½¬æ¢ä¸ºæµ®ç‚¹æ•°å†è½¬æ¢ä¸ºæ•´æ•°
                    'data': post_data
                }

                # æ’å…¥æ•°æ®åº“
                # Save to local storage
                local_data_manager.save_post(db_record)

                if result.data:
                    # æ•°æ®åº“ä¿å­˜æˆåŠŸï¼ˆä¸å†ç»´æŠ¤å†…å­˜ç¼“å­˜ï¼‰
                    return True
                else:
                    return False

            except Exception as e:
                error_str = str(e)

                # å¦‚æœæ˜¯é‡å¤é”®é”™è¯¯ï¼Œè¯´æ˜æ•°æ®å·²å­˜åœ¨ï¼Œè§†ä¸ºæˆåŠŸ
                if 'duplicate key value violates unique constraint' in error_str:
                    print(f"â„¹ï¸ å¸–å­å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œè·³è¿‡: {post_data['id']}")
                    return True

                if attempt < max_retries - 1:
                    print(f"âš ï¸ æ•°æ®åº“ä¿å­˜å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    print(f"ğŸ”„ ç­‰å¾… {(attempt + 1) * 2} ç§’åé‡è¯•...")
                    time.sleep((attempt + 1) * 2)  # é€’å¢ç­‰å¾…æ—¶é—´ï¼š2s, 4s, 6s
                else:
                    print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥ (å·²é‡è¯• {max_retries} æ¬¡): {e}")
                    return False

    def run_enhanced_scraping(self, max_posts=500, sort_type='hot', save_json=True, progress_callback=None):
        """è¿è¡Œå¢å¼ºç‰ˆçˆ¬å–"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆUofT Redditçˆ¬è™«")
        print("=" * 60)

        # è®¡ç®—éœ€è¦çˆ¬å–çš„æ–°å¸–å­æ•°é‡
        existing_count = self.get_database_post_count()
        needed_posts = max_posts  # ç”¨æˆ·æŒ‡å®šçš„å°±æ˜¯è¦çˆ¬å–çš„æ–°å¸–å­æ•°é‡

        print(f"ğŸ“Š ç›®æ ‡: çˆ¬å– {max_posts} ä¸ªæ–°å¸–å­ï¼Œæ•°æ®åº“ä¸­å·²æœ‰: {existing_count} ä¸ªå¸–å­")

        # å¦‚æœéœ€è¦çš„å¸–å­æ•°å¾ˆå°‘ï¼Œé€‚åº¦å¢åŠ æœç´¢èŒƒå›´ä»¥æé«˜æ‰¾åˆ°æ–°å¸–å­çš„æ¦‚ç‡
        # ä¼˜åŒ–ï¼šå¦‚æœæ•°æ®åº“ä¸ºç©ºï¼Œä¸éœ€è¦å¤ªå¤§çš„å€æ•°ï¼Œå› ä¸ºæ‰€æœ‰æ‰¾åˆ°çš„å¸–å­éƒ½æ˜¯æ–°çš„
        if existing_count == 0:
            search_multiplier = 1.2  # ç¨å¾®å¤šä¸€ç‚¹ç‚¹å³å¯
        elif needed_posts <= 10:
            # å¯¹äºå¾ˆå°‘çš„ç›®æ ‡ï¼Œæœç´¢æ›´å¤šå€™é€‰ä»¥æé«˜æˆåŠŸç‡
            search_multiplier = 7
        elif needed_posts <= 50:
            search_multiplier = 5
        elif needed_posts <= 200:
            search_multiplier = 3
        else:
            search_multiplier = 2

        actual_search_target = min(needed_posts * search_multiplier, 1000)  # æœ€å¤šæœç´¢1000ä¸ªå€™é€‰
        print(f"ğŸ” ä¸ºäº†æ‰¾åˆ° {needed_posts} ä¸ªæ–°å¸–å­ï¼Œå°†æœç´¢ {actual_search_target} ä¸ªå€™é€‰å¸–å­")
        self.target_posts = max_posts  # è®°å½•ç›®æ ‡æ•°é‡

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self.create_output_directory() if save_json else None

        # è·å–å¸–å­URLåˆ—è¡¨ï¼ˆä½¿ç”¨æ‰©å¤§çš„æœç´¢èŒƒå›´ï¼‰
        if sort_type.startswith('super'):
            # è¶…çº§æ¨¡å¼ï¼šä½¿ç”¨å¤šç­–ç•¥çªç ´APIé™åˆ¶
            print(f"ğŸš€ å¯ç”¨è¶…çº§æ¨¡å¼è·å–æµ·é‡å¸–å­ (æœç´¢: {actual_search_target} ä¸ªå€™é€‰)")
            post_urls = self.get_recent_posts_multi_strategy(actual_search_target, sort_type, progress_callback)
        elif sort_type == 'new' and needed_posts > 1000:
            # å¯¹äºå¤§é‡æ–°å¸–å­éœ€æ±‚ï¼Œä½¿ç”¨å¤šç­–ç•¥æ–¹æ³•
            print(f"ğŸš€ å¯ç”¨å¤šç­–ç•¥æ¨¡å¼è·å–å¤§é‡æœ€æ–°å¸–å­ (æœç´¢: {actual_search_target} ä¸ªå€™é€‰)")
            post_urls = self.get_recent_posts_multi_strategy(actual_search_target, "super_recent", progress_callback)
        else:
            # å¸¸è§„å•ä¸€ç­–ç•¥ï¼Œä½†ä¹Ÿä½¿ç”¨æ‰©å¤§çš„æœç´¢èŒƒå›´
            print(f"ğŸ”„ å¼€å§‹è·å– {sort_type} æ¨¡å¼ä¸‹çš„å¸–å­ (æœç´¢: {actual_search_target} ä¸ªå€™é€‰)...")
            post_urls = self.get_all_posts_paginated(actual_search_target, sort_type, progress_callback=progress_callback)

        # å¦‚æœå•ä¸€æ’åºæ–¹å¼è·å–ä¸å¤Ÿï¼Œå°è¯•å…¶ä»–æ’åºæ–¹å¼
        if len(post_urls) < needed_posts:
            remaining_needed = needed_posts - len(post_urls)
            print(f"âš ï¸ {sort_type} æ¨¡å¼åªè·å–åˆ° {len(post_urls)} ä¸ªå¸–å­ï¼Œè¿˜éœ€è¦ {remaining_needed} ä¸ª")

            # å°è¯•å…¶ä»–æ’åºæ–¹å¼ï¼ŒåŒ…æ‹¬ä¸åŒæ—¶é—´èŒƒå›´çš„top
            other_sorts = []
            if sort_type != 'new':
                other_sorts.append(('new', None))
            if sort_type != 'best':
                other_sorts.append(('best', None))
            if sort_type != 'top':
                other_sorts.extend([('top', 'all'), ('top', 'year'), ('top', 'month'), ('top', 'week')])
            if sort_type != 'rising':
                other_sorts.append(('rising', None))

            for backup_sort, time_filter in other_sorts:
                if len(post_urls) >= needed_posts:
                    break

                sort_desc = f"{backup_sort}({time_filter})" if time_filter else backup_sort
                print(f"ğŸ”„ å°è¯• {sort_desc} æ¨¡å¼è·å–æ›´å¤šå¸–å­...")
                backup_urls = self.get_all_posts_paginated(remaining_needed, backup_sort, time_filter, progress_callback=progress_callback)

                if backup_urls:
                    post_urls.extend(backup_urls)
                    remaining_needed = needed_posts - len(post_urls)
                    print(f"âœ… {backup_sort} æ¨¡å¼æ–°å¢ {len(backup_urls)} ä¸ªå¸–å­ï¼Œæ€»è®¡: {len(post_urls)}")

                    if len(post_urls) >= needed_posts:
                        post_urls = post_urls[:needed_posts]  # æˆªå–åˆ°ç›®æ ‡æ•°é‡
                        break

        if not post_urls:
            print("âŒ æ²¡æœ‰è·å–åˆ°æ–°çš„å¸–å­URL")
            return

        print(f"ğŸ“Š è·å–åˆ° {len(post_urls)} ä¸ªå€™é€‰å¸–å­URL")

        # æ‰¹é‡è¿‡æ»¤å·²å­˜åœ¨çš„å¸–å­ï¼ˆæœ€ç»ˆå»é‡æ£€æŸ¥ï¼‰
        print("ğŸ” æ‰§è¡Œæœ€ç»ˆå»é‡æ£€æŸ¥...")
        post_urls = self.filter_new_posts_batch(post_urls)
        print(f"âœ… å»é‡åå‰©ä½™ {len(post_urls)} ä¸ªæ–°å¸–å­å¾…å¤„ç†")

        if len(post_urls) == 0:
            print("âš ï¸ æ‰€æœ‰å€™é€‰å¸–å­éƒ½å·²å­˜åœ¨äºæ•°æ®åº“ä¸­")
            print("ğŸ’¡ å»ºè®®ï¼šå°è¯•ä¸åŒçš„ç­–ç•¥æˆ–ç­‰å¾…æ–°å¸–å­å‘å¸ƒ")
            return

        print(f"\nğŸ¯ å¼€å§‹æŠ“å– {len(post_urls)} ä¸ªå¸–å­...")
        print("=" * 60)

        scraped_posts = []  # ç”¨äºæœ€ç»ˆç»Ÿè®¡å’Œçƒ­é—¨å¸–å­åˆ†æ
        posts_to_save = []  # æ‰¹é‡ä¿å­˜ç¼“å†²åŒº
        start_time = time.time()
        # å½“ç›®æ ‡å¾ˆå°æ—¶ï¼Œä½¿ç”¨æ›´å°çš„æ‰¹é‡å¤§å°ä»¥ä¾¿åŠæ—¶åœæ­¢
        BATCH_SIZE = min(10, needed_posts) if needed_posts <= 20 else 50

        for i, (post_url, post_id) in enumerate(post_urls, 1):
            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡å¸–å­æ•°
            if self.scraped_count >= max_posts:
                print(f"\nğŸ‰ å·²è¾¾åˆ°ç›®æ ‡ï¼å·²æˆåŠŸçˆ¬å– {self.scraped_count} ä¸ªæ–°å¸–å­ï¼Œåœæ­¢å¤„ç†")
                break

            print(f"\n[{i}/{len(post_urls)}] å¤„ç†å¸–å­: {post_id}")

            # æŠ“å–å¸–å­æ•°æ®
            post_data = self.scrape_post_with_comments(post_url, post_id)

            if post_data:
                # ä¿ç•™ç”¨äºç»Ÿè®¡
                scraped_posts.append(post_data)
                self.scraped_count += 1

                # å¯é€‰ï¼šä¿å­˜JSONæ–‡ä»¶
                json_status = ""
                if save_json and output_dir:
                    try:
                        title_clean = self.sanitize_filename(post_data['title'], 60)
                        filename = f"{output_dir}/{post_data['id']}_{title_clean}.json"

                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(post_data, f, ensure_ascii=False, indent=2)

                        json_status = "ğŸ’¾ JSONå·²ä¿å­˜"
                    except Exception as e:
                        json_status = f"âš ï¸ JSONä¿å­˜å¤±è´¥: {str(e)[:30]}..."

                # çŠ¶æ€æ˜¾ç¤º
                print(f"âœ… {post_data['title'][:50]}...")
                print(f"    ğŸ‘¤ ä½œè€…: {post_data['author']} | ğŸ“ˆ åˆ†æ•°: {post_data['score']} | ğŸ’¬ è¯„è®º: {len(post_data['comments'])}")
                if json_status:
                    print(f"    {json_status}")
                
                # å‘é€è¿›åº¦å›è°ƒ
                if progress_callback:
                    try:
                        progress_callback(self.scraped_count, max_posts, f"Scraped: {post_data['title'][:30]}...")
                    except Exception as e:
                        print(f"âš ï¸ Progress callback failed: {e}")

                # å½“ç›®æ ‡å¾ˆå°æ—¶ï¼Œç«‹å³ä¿å­˜æ¯ä¸ªå¸–å­ä»¥ä¾¿åŠæ—¶åœæ­¢
                if needed_posts <= 20:
                    # å°ç›®æ ‡ï¼šç«‹å³ä¿å­˜
                    print(f"ğŸ”„ ç«‹å³ä¿å­˜å¸–å­åˆ°æ•°æ®åº“...")
                    if local_data_manager.save_post(post_data):
                        print(f"âœ… ä¿å­˜æˆåŠŸ")
                        if self.scraped_count >= max_posts:
                            print(f"ğŸ‰ å·²è¾¾åˆ°ç›®æ ‡ï¼å·²æˆåŠŸçˆ¬å– {self.scraped_count} ä¸ªæ–°å¸–å­ï¼Œåœæ­¢å¤„ç†")
                            break
                    else:
                        print(f"âŒ ä¿å­˜å¤±è´¥")
                else:
                    # å¤§ç›®æ ‡ï¼šæ‰¹é‡ä¿å­˜
                    posts_to_save.append(post_data)
                    if len(posts_to_save) >= BATCH_SIZE or i == len(post_urls):
                        print(f"ğŸ”„ æ‰¹é‡ä¿å­˜ {len(posts_to_save)} ä¸ªå¸–å­åˆ°æ•°æ®åº“...")
                        saved_count = local_data_manager.save_posts_batch(posts_to_save)
                        if saved_count > 0:
                            print(f"âœ… æ‰¹é‡ä¿å­˜æˆåŠŸ: {saved_count}/{len(posts_to_save)} ä¸ªå¸–å­")

                            # ä¿å­˜åå†æ¬¡æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                            if self.scraped_count >= max_posts:
                                print(f"ğŸ‰ å·²è¾¾åˆ°ç›®æ ‡ï¼å·²æˆåŠŸçˆ¬å– {self.scraped_count} ä¸ªæ–°å¸–å­ï¼Œåœæ­¢å¤„ç†")
                                break
                        else:
                            print(f"âŒ æ‰¹é‡ä¿å­˜å¤±è´¥")
                        posts_to_save = []  # æ¸…ç©ºç¼“å†²åŒº

            # åŠ¨æ€è¿›åº¦æ˜¾ç¤º - æ ¹æ®æ€»æ•°è°ƒæ•´é¢‘ç‡
            progress_interval = max(10, len(post_urls) // 100)  # è‡³å°‘æ¯10ä¸ªï¼Œæœ€å¤š100æ¬¡æŠ¥å‘Š
            if i % progress_interval == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / i
                remaining = (len(post_urls) - i) * avg_time

                # è®¡ç®—æˆåŠŸç‡
                success_rate = (self.scraped_count / i) * 100 if i > 0 else 0

                print(f"\nğŸ“Š è¿›åº¦æŠ¥å‘Š: {i}/{len(post_urls)} ({i/len(post_urls)*100:.1f}%)")
                print(f"   âš¡ å¹³å‡é€Ÿåº¦: {avg_time:.1f}s/å¸– | æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"   â° é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ | å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")

            # ä½¿ç”¨æ™ºèƒ½é€Ÿç‡æ§åˆ¶å™¨çš„å»¶è¿Ÿ
            if i < len(post_urls):
                delay = self.rate_controller.get_delay()
                # æ·»åŠ ä¸€äº›éšæœºæ€§ä»¥æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                delay = random.uniform(delay * 0.8, delay * 1.2)
                time.sleep(delay)

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•å…¶ä»–ç­–ç•¥
        current_count = self.get_database_post_count()
        if current_count < max_posts and len(post_urls) > 0:
            remaining_needed = max_posts - current_count
            print(f"\nğŸ¯ å½“å‰æ•°æ®åº“æœ‰ {current_count} ä¸ªå¸–å­ï¼Œè·ç¦»ç›®æ ‡ {max_posts} è¿˜éœ€è¦ {remaining_needed} ä¸ª")

            if remaining_needed > 0:
                print("ğŸ”„ å°è¯•å…¶ä»–ç­–ç•¥è·å–æ›´å¤šå¸–å­...")

                # å°è¯•ä¸åŒçš„ç­–ç•¥
                backup_strategies = ['new', 'top', 'rising']
                for backup_strategy in backup_strategies:
                    if backup_strategy != sort_type:  # é¿å…é‡å¤ç›¸åŒç­–ç•¥
                        print(f"ğŸ”„ å°è¯• {backup_strategy} ç­–ç•¥...")
                        backup_urls = self.get_all_posts_paginated(remaining_needed * 3, backup_strategy)
                        backup_urls = self.filter_new_posts_batch(backup_urls)

                        if backup_urls:
                            print(f"âœ… {backup_strategy} ç­–ç•¥æ‰¾åˆ° {len(backup_urls)} ä¸ªæ–°å¸–å­")
                            # å¤„ç†è¿™äº›æ–°å¸–å­ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸é‡å¤æ‰€æœ‰é€»è¾‘ï¼‰
                            for post_url, post_id in backup_urls[:remaining_needed]:
                                post_data = self.scrape_post_with_comments(post_url, post_id)
                                if post_data:
                                    local_data_manager.save_post(post_data)
                                    scraped_posts.append(post_data)
                                    self.scraped_count += 1
                                    print(f"âœ… é¢å¤–æŠ“å–: {post_data['title'][:50]}...")

                                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                                    current_count = self.get_database_post_count()
                                    if current_count >= max_posts:
                                        print(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡ï¼æ•°æ®åº“ç°æœ‰ {current_count} ä¸ªå¸–å­")
                                        break
                            break

        # æœ€ç»ˆç»Ÿè®¡
        self.print_final_stats(scraped_posts, output_dir, start_time)
        
        return {
            'scraped_posts': scraped_posts,
            'output_dir': output_dir
        }

    def print_final_stats(self, scraped_posts, output_dir, start_time):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        elapsed_time = time.time() - start_time

        print("\n" + "=" * 60)
        print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print("=" * 60)

        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   âœ… æˆåŠŸæŠ“å–: {self.scraped_count} ä¸ªå¸–å­")
        print(f"   â­ï¸  è·³è¿‡é‡å¤: {self.skipped_count} ä¸ªå¸–å­")
        print(f"   âŒ å¤±è´¥: {self.error_count} ä¸ªå¸–å­")
        print(f"   â±ï¸  æ€»è€—æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ")
        print(f"   âš¡ å¹³å‡é€Ÿåº¦: {elapsed_time/max(1, self.scraped_count):.1f} ç§’/å¸–")

        if output_dir:
            print(f"   ğŸ“ JSONæ–‡ä»¶ä¿å­˜åœ¨: {output_dir}/")

        # æ•°æ®åº“ç»Ÿè®¡
        try:
            total_in_db = local_data_manager.get_posts_count()
            print(f"   ğŸ—„ï¸  æ•°æ®åº“æ€»å¸–å­æ•°: {total_in_db}")
            if hasattr(self, 'target_posts'):
                completion = min(100, (self.scraped_count / self.target_posts) * 100)
                print(f"   ğŸ¯ ç›®æ ‡å®Œæˆåº¦: {self.scraped_count}/{self.target_posts} ({completion:.1f}%)")
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•è·å–æ•°æ®åº“ç»Ÿè®¡: {e}")

        # çƒ­é—¨å¸–å­ç»Ÿè®¡
        if scraped_posts:
            top_post = max(scraped_posts, key=lambda x: x['score'])
            most_commented = max(scraped_posts, key=lambda x: len(x['comments']))

            print(f"\nğŸ† æœ¬æ¬¡çˆ¬å–äº®ç‚¹:")
            print(f"   ğŸ“ˆ æœ€é«˜åˆ†å¸–å­: {top_post['title'][:40]}... (åˆ†æ•°: {top_post['score']})")
            print(f"   ğŸ’¬ æœ€å¤šè¯„è®ºå¸–å­: {most_commented['title'][:40]}... (è¯„è®º: {len(most_commented['comments'])})")

        print("\nğŸ”— å¯åœ¨Supabase Webç•Œé¢æŸ¥çœ‹å’ŒæŸ¥è¯¢æ‰€æœ‰æ•°æ®")
        print("=" * 60)

    def run_scraping_session(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Core scraping function for integration with reddit_system.py

        Args:
            config: Configuration dictionary with keys:
                - max_posts: Maximum number of posts to scrape
                - strategy: Scraping strategy ('super_full', 'super_recent', etc.)
                - save_json: Whether to save JSON files (default: False)
                - mode: 'incremental' or 'fresh'

        Returns:
            Dict with detailed results
        """
        try:
            # Extract configuration
            max_posts = config.get('max_posts', 500)
            strategy = config.get('strategy', 'super_full')
            save_json = config.get('save_json', False)
            mode = config.get('mode', 'incremental')
            subreddit = config.get('subreddit', 'UofT')

            # Determine if super mode should be used
            super_mode = max_posts > 2000 or strategy.startswith('super_')

            print(f"ğŸ“Š é…ç½®: {max_posts} å¸–å­, {'è¶…çº§æ¨¡å¼' if super_mode else 'æ™®é€šæ¨¡å¼'}, ç­–ç•¥: {strategy}")

            # Initialize counters
            initial_count = self.get_database_post_count()

            # Run the core scraping logic using the existing run_enhanced_scraping method
            # Note: We need to ensure the instance uses the correct subreddit
            # Since we are inside the instance method, we assume self was initialized correctly
            # But run_scraping_session is often called on a fresh instance or via module function
            # If this instance was initialized with default 'UofT', we might need to re-init or just use it
            
            # Actually, the module-level function creates the instance.
            # We should update the module-level function to pass the subreddit.
            scraping_result = self.run_enhanced_scraping(
                max_posts=max_posts, 
                sort_type=strategy, 
                save_json=save_json,
                progress_callback=config.get('progress_callback')
            )

            # Calculate results from database count
            final_count = self.get_database_post_count()
            scraped_count = final_count - initial_count
            
            # Extract scraped posts and output dir
            scraped_posts = []
            output_dir = None
            if isinstance(scraping_result, dict):
                scraped_posts = scraping_result.get('scraped_posts', [])
                output_dir = scraping_result.get('output_dir')
            
            # Generate summary Markdown file
            file_path = None
            if scraped_posts:
                # If no output_dir (save_json=False), create a default one
                if not output_dir:
                    script_dir = os.path.dirname(os.path.abspath(__file__))
                    project_root = os.path.dirname(os.path.dirname(script_dir))
                    output_dir = os.path.join(project_root, 'output', 'reddit', 'latest')
                    os.makedirs(output_dir, exist_ok=True)
                
                file_path = os.path.join(output_dir, "index.md")
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(f"# Reddit Scrape Results: r/{subreddit}\n\n")
                        f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"**Strategy:** {strategy}\n")
                        f.write(f"**Posts Scraped:** {len(scraped_posts)}\n\n")
                        
                        for i, post in enumerate(scraped_posts, 1):
                            f.write(f"## {i}. {post.get('title', 'Untitled')}\n\n")
                            f.write(f"**Author:** u/{post.get('author', 'unknown')} | **Score:** {post.get('score', 0)}\n")
                            f.write(f"**URL:** {post.get('url', '')}\n\n")
                            if post.get('selftext'):
                                summary = post['selftext'][:200].replace('\n', ' ') + "..." if len(post['selftext']) > 200 else post['selftext']
                                f.write(f"{summary}\n\n")
                            f.write("---\n\n")
                    print(f"ğŸ“„ Generated summary markdown: {file_path}")
                except Exception as e:
                    print(f"âš ï¸ Failed to generate markdown summary: {e}")

            return {
                'status': 'success',
                'scraped_count': scraped_count,
                'total_posts_in_db': final_count,
                'strategy_used': strategy,
                'super_mode': super_mode,
                'file_path': file_path,
                'message': f'Successfully scraped {scraped_count} new posts using {strategy} strategy'
            }

        except Exception as e:
            # Get current count for partial success reporting
            try:
                current_count = self.get_database_post_count()
                initial_count = getattr(self, '_initial_count', current_count)
                scraped_count = current_count - initial_count
            except:
                scraped_count = 0

            return {
                'status': 'error',
                'scraped_count': scraped_count,
                'message': f'Scraping failed: {str(e)}'
            }


# Module-level convenience function for easy integration
def run_scraping_session(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convenience function to run a scraping session without creating an instance

    Args:
        config: Configuration dictionary

    Returns:
        Dict with scraping results
    """
    subreddit = config.get('subreddit', 'UofT')
    scraper = EnhancedUofTScraper(target_subreddit=subreddit)
    return scraper.run_scraping_session(config)


if __name__ == "__main__":
    print("ğŸ¤– å¢å¼ºç‰ˆUofT Redditçˆ¬è™«")
    print("æ”¯æŒå¤§è§„æ¨¡çˆ¬å–ã€è‡ªåŠ¨å»é‡ã€çŠ¶æ€è®°å½•")
    print("=" * 60)

    # ç”¨æˆ·é…ç½®
    try:
        max_posts = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„æœ€å¤§å¸–å­æ•° (é»˜è®¤500): ") or "500")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¶…çº§æ¨¡å¼ - é»˜è®¤å¯ç”¨è¶…çº§æ¨¡å¼
        if max_posts > 2000:
            super_mode = input(f"ğŸš€ æ£€æµ‹åˆ°å¤§é‡éœ€æ±‚({max_posts}ä¸ª)ï¼Œæ˜¯å¦å¯ç”¨è¶…çº§æ¨¡å¼? (y/n, é»˜è®¤y): ").lower().strip()
            super_mode = super_mode != 'n'  # é»˜è®¤å¯ç”¨
        else:
            super_mode = input("ğŸš€ æ˜¯å¦å¯ç”¨è¶…çº§æ¨¡å¼çªç ´APIé™åˆ¶? (y/n, é»˜è®¤y): ").lower().strip()
            super_mode = super_mode != 'n'  # é»˜è®¤å¯ç”¨è¶…çº§æ¨¡å¼

        if not super_mode:
            # æ™®é€šæ¨¡å¼ï¼šè®©ç”¨æˆ·é€‰æ‹©æ’åºæ–¹å¼
            sort_type = input("æ’åºæ–¹å¼ (hot/new/best/top/rising, é»˜è®¤new): ").strip() or "new"
        else:
            # è¶…çº§æ¨¡å¼ï¼šè®©ç”¨æˆ·é€‰æ‹©é‡ç‚¹ç­–ç•¥
            print("\nğŸ¯ è¶…çº§æ¨¡å¼ç­–ç•¥é€‰æ‹©:")
            print("1. å…¨é¢æ¨¡å¼ - ä½¿ç”¨æ‰€æœ‰ç­–ç•¥ (æ¨è)")
            print("2. æ—¶æ•ˆä¼˜å…ˆ - é‡ç‚¹è·å–æœ€æ–°å¸–å­")
            print("3. çƒ­é—¨ä¼˜å…ˆ - é‡ç‚¹è·å–é«˜åˆ†å¸–å­")
            print("4. æœç´¢ä¼˜å…ˆ - é‡ç‚¹ä½¿ç”¨å…³é”®è¯æœç´¢")

            strategy_choice = input("è¯·é€‰æ‹©ç­–ç•¥ (1-4, é»˜è®¤1-å…¨é¢æ¨¡å¼): ").strip() or "1"
            strategy_map = {
                "1": "super_full",
                "2": "super_recent",
                "3": "super_popular",
                "4": "super_search"
            }
            sort_type = strategy_map.get(strategy_choice, "super_full")

        save_json = input("æ˜¯å¦åŒæ—¶ä¿å­˜JSONæ–‡ä»¶? (y/n, é»˜è®¤n): ").lower().strip() == 'y'
        fast_mode = True  # é»˜è®¤å¯ç”¨å¿«é€Ÿæ¨¡å¼ï¼Œä¸å†è¯¢é—®

        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        if super_mode:
            strategy_names = {
                "super_full": "å…¨é¢æ¨¡å¼",
                "super_recent": "æ—¶æ•ˆä¼˜å…ˆ",
                "super_popular": "çƒ­é—¨ä¼˜å…ˆ",
                "super_search": "æœç´¢ä¼˜å…ˆ"
            }
            mode_desc = f"è¶…çº§æ¨¡å¼-{strategy_names.get(sort_type, 'å…¨é¢æ¨¡å¼')}"
        else:
            mode_desc = f"{sort_type}æ’åº"

        print(f"\nğŸ¯ é…ç½®: æœ€å¤š{max_posts}ä¸ªå¸–å­, {mode_desc}, JSONä¿å­˜: {'æ˜¯' if save_json else 'å¦'}, å¿«é€Ÿæ¨¡å¼: å·²å¯ç”¨ âš¡")

        if super_mode:
            strategy_descriptions = {
                "super_full": [
                    "ğŸš€ PRAWå®˜æ–¹APIå¢å¼ºè·å– (çªç ´JSON APIé™åˆ¶)",
                    "ğŸ“Š æ‰€æœ‰æ—¶é—´èŒƒå›´çš„topæ’åº (day/week/month/year/all)",
                    "ğŸ”„ æ‰€æœ‰æ’åºæ–¹å¼ (hot/new/rising/best)",
                    "ğŸ›ï¸ æ·±åº¦å†å²æŒ–æ˜ (controversial/gildedç­‰ç‰¹æ®Šæ’åº)",
                    "ğŸ” é«˜çº§ç»„åˆæœç´¢ (30+ä¸ªå¤æ‚æŸ¥è¯¢ç»„åˆ)",
                    "ğŸ¯ æ™ºèƒ½å…³é”®è¯æœç´¢ (50+ä¸ªä¸“ä¸šå…³é”®è¯ï¼Œæ™ºèƒ½å»é‡)"
                ],
                "super_recent": [
                    "â° é‡ç‚¹è·å–æœ€æ–°å¸–å­ (day/week/month/year topæ’åº)",
                    "ğŸ†• ä¼˜å…ˆä½¿ç”¨newå’Œrisingæ’åº",
                    "ğŸ›ï¸ æ·±åº¦å†å²æŒ–æ˜ (é¿å…é‡å¤ç­–ç•¥)",
                    "ğŸ” æ™ºèƒ½å…³é”®è¯æœç´¢ (50+ä¸ªUofTä¸“ä¸šå…³é”®è¯)"
                ],
                "super_popular": [
                    "ğŸ”¥ é‡ç‚¹è·å–çƒ­é—¨å¸–å­ (hot/bestæ’åºä¼˜å…ˆ)",
                    "ğŸ“Š å…¨æ—¶é—´èŒƒå›´topæ’åº (all/year/month)",
                    "ğŸ“š æ·±åº¦åˆ†é¡µè·å–é«˜åˆ†å¸–å­",
                    "ğŸ” å…³é”®è¯æœç´¢è¡¥å……"
                ],
                "super_search": [
                    "ğŸ” ä¼˜å…ˆä½¿ç”¨å…³é”®è¯æœç´¢ (25ä¸ªUofTå…³é”®è¯)",
                    "ğŸ“Š ä¸­æœŸæ—¶é—´èŒƒå›´topæ’åº (month/year)",
                    "ğŸ”„ åŸºç¡€æ’åºæ–¹å¼ (hot/new)",
                    "ğŸ“š æ·±åº¦åˆ†é¡µè¡¥å……"
                ]
            }

            print(f"ğŸš€ {strategy_names.get(sort_type, 'å…¨é¢æ¨¡å¼')}å°†ä½¿ç”¨ä»¥ä¸‹ç­–ç•¥:")
            for desc in strategy_descriptions.get(sort_type, strategy_descriptions["super_full"]):
                print(f"   {desc}")

        print("=" * 60)

        # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶è¿è¡Œ
        scraper = EnhancedUofTScraper()
        # å¿«é€Ÿæ¨¡å¼é»˜è®¤å¯ç”¨ - é…ç½®æ™ºèƒ½é€Ÿç‡æ§åˆ¶å™¨
        scraper.rate_controller.base_delay = 1.0
        scraper.rate_controller.current_delay = 1.0
        print("âš¡ å¿«é€Ÿæ¨¡å¼å·²è‡ªåŠ¨å¯ç”¨ï¼šå»¶è¿Ÿå‡å°‘åˆ°1ç§’ï¼Œé€Ÿåº¦æå‡2å€ï¼")
        scraper.run_enhanced_scraping(max_posts, sort_type, save_json)

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºé”™è¯¯: {e}")
        sys.exit(1)
