"""
Paginated Scraping Strategy

Basic pagination through Reddit's JSON API.
"""

import time
import requests
from typing import List, Tuple, Optional, Callable
from .base import ScrapingStrategy


class PaginatedStrategy(ScrapingStrategy):
    """åˆ†é¡µæŠ“å–ç­–ç•¥"""
    
    def fetch_posts(
        self,
        target: str,
        max_posts: int,
        progress_callback: Optional[Callable] = None,
        sort_type: str = 'hot',
        time_filter: str = 'all',
        is_user_mode: bool = False
    ) -> List[Tuple[str, str]]:
        """
        Fetch posts using pagination
        
        Args:
            target: Subreddit name or username
            max_posts: Maximum posts to fetch
            progress_callback: Progress callback
            sort_type: Sort type (hot/new/top/best/rising)
            time_filter: Time filter for 'top' sort
            is_user_mode: Whether scraping a user profile
            
        Returns:
            List of (post_url, post_id) tuples
        """
        post_urls = []
        after = None
        page = 1
        consecutive_timeouts = 0
        consecutive_errors = 0
        MAX_CONSECUTIVE_TIMEOUTS = 2 if is_user_mode else 3
        MAX_CONSECUTIVE_ERRORS = 3

        while len(post_urls) < max_posts:
            # Construct API URL
            if is_user_mode:
                api_url = f"https://www.reddit.com/user/{target}/overview.json?limit=100&sort={sort_type}"
            else:
                api_url = f"https://www.reddit.com/r/{target}/{sort_type}.json?limit=100"
            
            if sort_type == 'top' and time_filter:
                api_url += f"&t={time_filter}"
            if after:
                api_url += f"&after={after}"

            try:
                print(f"ğŸ“„ è·å–ç¬¬ {page} é¡µ...", end=" ", flush=True)
                
                # Rate limiting
                base_delay = self.rate_controller.get_delay()
                delay = base_delay + (page % 3) * 0.5
                if delay > 1:
                    print(f"(å»¶è¿Ÿ {delay:.1f}s)", end=" ", flush=True)
                time.sleep(delay)

                print(f"ğŸŒ Requesting: {api_url}", end=" ", flush=True)
                response = self.session.get_session().get(api_url, timeout=10)
                print(f"â¬…ï¸ Status: {response.status_code}", end=" ", flush=True)

                if response.status_code == 403:
                    print("âŒ è¢«é˜»æ­¢")
                    break
                elif response.status_code == 429:
                    print("âš ï¸ é‡åˆ°é™æµ")
                    self._handle_rate_limit()
                    continue
                
                # Check JSON response
                content_type = response.headers.get('Content-Type', '')
                if 'application/json' not in content_type:
                    print(f"âš ï¸ éJSONå“åº”")
                    break

                self.rate_controller.record_success()
                consecutive_timeouts = 0
                consecutive_errors = 0
                response.raise_for_status()
                
                
                try:
                    print("ğŸ“¦ Parsing JSON...", end=" ", flush=True)
                    data = response.json()
                    print("âœ“", end=" ", flush=True)
                except ValueError as e:
                    print(f"\nâŒ JSONè§£æå¤±è´¥: {e}")
                    break
                
                try:
                    posts = data['data']['children']
                    print(f"Found {len(posts)} posts", end=" ", flush=True)
                except (KeyError, TypeError) as e:
                    print(f"\nâŒ æ•°æ®ç»“æ„é”™è¯¯: {e}")
                    print(f"Data keys: {data.keys() if isinstance(data, dict) else 'N/A'}")
                    break

                if not posts:
                    print("\nâœ… å·²è·å–æ‰€æœ‰å¯ç”¨å¸–å­", flush=True)
                    break

                new_posts = 0
                print(f"Processing {len(posts)} posts...", end=" ", flush=True)
                
                try:
                    for idx, post in enumerate(posts, 1):
                        try:
                            if len(post_urls) >= max_posts:
                                break
                            
                            try:
                                post_data = post['data']
                                post_id = post_data['id']
                                post_url = f"https://www.reddit.com{post_data['permalink']}"
                                post_urls.append((post_url, post_id))
                                new_posts += 1
                                
                                # Show progress every 25 posts
                                if idx % 25 == 0:
                                    print(f"{idx}...", end=" ", flush=True)
                            except (KeyError, TypeError) as e:
                                print(f"\nâš ï¸ è·³è¿‡æ ¼å¼å¼‚å¸¸çš„å¸–å­ #{idx}: {e}", flush=True)
                                continue
                        except Exception as e:
                            print(f"\nâŒ å¤„ç†å¸–å­ #{idx} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {type(e).__name__}: {e}", flush=True)
                            continue
                except Exception as e:
                    print(f"\nâŒ enumerateå¾ªç¯é”™è¯¯: {type(e).__name__}: {e}", flush=True)
                    break

                print(f"æ–°å¢ {new_posts} ä¸ª", flush=True)

                # Optimization: If we got fewer posts than the limit (100), we've reached the end
                if len(posts) < 100:
                    print(f" âœ… æœ¬é¡µåªæœ‰ {len(posts)} ä¸ªå¸–å­ (<100)ï¼Œå·²åˆ°è¾¾æœ«å°¾", flush=True)
                    break

                # Get next page
                after = data['data']['after']
                if not after:
                    print(" âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ", flush=True)
                    break

                page += 1
            
            except requests.exceptions.Timeout:
                consecutive_timeouts += 1
                print(f"â±ï¸ è¯·æ±‚è¶…æ—¶ (10s) [è¿ç»­{consecutive_timeouts}æ¬¡]", flush=True)
                if consecutive_timeouts >= 3:
                    print("âŒ è¿ç»­è¶…æ—¶è¿‡å¤šï¼Œæ”¾å¼ƒ", flush=True)
                    break
                time.sleep(2)
                        
            except Exception as e:
                consecutive_errors += 1
                print(f"âŒ ç¬¬ {page} é¡µè·å–å¤±è´¥: {e} [è¿ç»­{consecutive_errors}æ¬¡]", flush=True)
                
                if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:
                    if len(post_urls) > 0:
                        print(f"ğŸ’¡ è¿ç»­é”™è¯¯{MAX_CONSECUTIVE_ERRORS}æ¬¡ï¼Œåœæ­¢æœç´¢", flush=True)
                        break
                    else:
                        break
                else:
                    if len(post_urls) > 0 and after:
                        print(f"ğŸ’¡ è·³è¿‡æ­¤é¡µç»§ç»­", flush=True)
                        page += 1
                        continue
                    else:
                        break

        print(f"\n ğŸ“Š æ€»å…±è·å–åˆ° {len(post_urls)} ä¸ªURL", flush=True)
        return post_urls
