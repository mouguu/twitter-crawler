#!/usr/bin/env node

/**
 * Twitter/X Crawler CLI
 * ä¸“æ³¨äºæŠ“å–Twitter/Xè´¦å·ä¿¡æ¯ä¸æ¨æ–‡
 */

const path = require('path');
const fs = require('fs');
const { Command } = require('commander');
const scraper = require('./scrape-unified');
const fileUtils = require('./utils/fileutils');
const markdownUtils = require('./utils/markdown');
const timeUtils = require('./utils/time');
// const mergeUtils = require('./utils/merge');

// åˆ›å»ºå‘½ä»¤è¡Œç¨‹åº
const program = new Command();

// ç‰ˆæœ¬å’Œæè¿°
program
  .name('twitter-crawler')
  .description('Twitter/X Crawler - CLI tool for scraping Twitter/X content')
  .version('1.0.0');

// é€šç”¨é€‰é¡¹
program
  .option('-d, --debug', 'Enable debug mode with verbose logs')
  .option('-o, --output <dir>', 'Output directory', './output')
  .option('-m, --merge', 'Merge all results into a single file', false)
  .option('--merge-file <filename>', 'Merge file name', 'merged')
.option('--format <format>', 'Export format: md/json/csv', 'md');

// Twitterå‘½ä»¤
program
  .command('twitter')
  .description('Scrape Twitter/X account information and tweets')
  .option('-u, --username <username>', 'Twitter username (without @)')
  .option('-U, --url <profileUrl>', 'Twitter/X profile URL (e.g., https://x.com/elonmusk)')
  .option('-f, --file <filepath>', 'File containing Twitter usernames (one per line)')
  .option('-c, --count <number>', 'Number of tweets to scrape per account', '20')
  .option('-s, --separate', 'Save each Twitter account separately', false)
  .option('--with-replies', 'Scrape with_replies tab (saved with same logic)', false)
  .option('--json', 'Additionally export as JSON (consolidated into one file)', false)
  .option('--csv', 'Additionally export as CSV (consolidated into one file)', false)
  .option('--headless <boolean>', 'Run browser in headless mode', 'true')
  .option('-o, --output <dir>', 'Output directory', './output')
  .option('--timezone <timezone>', 'Timezone for timestamp output (IANA name)')
  .option('-d, --debug', 'Enable debug mode with verbose logs')
  .option('-m, --merge', 'Merge all results into a single file', false)
  .option('--merge-file <filename>', 'Merge file name', 'merged')
  .option('--format <format>', 'Export format: md/json/csv', 'md')
  .action(async (options) => {
    try {
      // éªŒè¯å¹¶åˆå§‹åŒ–é€‰é¡¹
      if (!options.username && !options.url && !options.file) {
        console.error('Error: Please provide Twitter username, profile URL, or file containing usernames/URLs');
        process.exit(1);
      }
      
      options.count = parseInt(options.count);
      options.headless = options.headless === 'true';
      const outputDir = path.resolve(options.output || './output');
      const timezoneInput = options.timezone || timeUtils.getDefaultTimezone();
      const timezone = timeUtils.resolveTimezone(timezoneInput);
      
      // ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
      try {
        await fileUtils.ensureDirExists(outputDir);
      } catch (error) {
        console.error(`Failed to create output directory: ${outputDir}`, error);
        process.exit(1);
      }

      console.log('ğŸš€ Starting Twitter scraping task...');
      console.log(`â±ï¸ Using timezone: ${timezone}`);
      
      // è¾…åŠ©å‡½æ•°: å½’ä¸€åŒ–è¾“å…¥ä¸ºç”¨æˆ·å
      const normalizeToUsername = (input) => {
        if (!input) return null;
        const raw = String(input).trim();
        if (!raw) return null;
        // 1) å¤„ç† @handle
        if (raw.startsWith('@')) return raw.slice(1);
        // 2) å¤„ç† URL
        if (/^https?:\/\//i.test(raw)) {
          try {
            const u = new URL(raw);
            // ä»…æ¥å— x.com æˆ– twitter.com
            if (!/(^|\.)x\.com$|(^|\.)twitter\.com$/i.test(u.hostname)) return null;
            // å–è·¯å¾„ç¬¬ä¸€ä¸ªéç©ºæ®µ
            const seg = u.pathname.split('/').filter(Boolean)[0] || '';
            // æ’é™¤éç”¨æˆ·è·¯å¾„
            const blocked = new Set(['home','explore','i','notifications','messages','settings','search']);
            if (!seg || blocked.has(seg.toLowerCase())) return null;
            return seg.replace(/^@/, '');
          } catch (_) {
            return null;
          }
        }
        // 3) æ™®é€šç”¨æˆ·å
        return raw.replace(/^@/, '');
      };

      // æ£€æµ‹æ˜¯å¦è¯·æ±‚äº† with_replies æ ‡ç­¾
      const isWithReplies = (input) => {
        if (!input) return false;
        const raw = String(input).trim().toLowerCase();
        if (!raw) return false;
        if (/^https?:\/\//i.test(raw)) {
          try {
            const u = new URL(raw);
            const pathLower = u.pathname.toLowerCase();
            return pathLower.includes('/with_replies');
          } catch (_) {
            return false;
          }
        }
        return false;
      };

      // åˆå§‹åŒ–ç”¨æˆ·åˆ—è¡¨
      let usernames = [];
      let withReplies = !!options.withReplies;
      if (options.username) {
        const u = normalizeToUsername(options.username);
        if (u) usernames.push(u);
      }
      if (options.url) {
        const u = normalizeToUsername(options.url);
        if (u) usernames.push(u);
        if (isWithReplies(options.url)) withReplies = true;
      } else if (options.file && fs.existsSync(options.file)) {
        const fileContent = fs.readFileSync(options.file, 'utf8');
        const lines = fileContent.split('\n');
        usernames = lines
          .map(line => normalizeToUsername(line))
          .filter(line => line && !String(line).startsWith('#'));
        // å¦‚æœæ–‡ä»¶é‡Œä»»ä¸€è¡ŒåŒ…å« with_repliesï¼Œåˆ™å¯ç”¨
        if (!withReplies) {
          withReplies = lines.some(line => isWithReplies(line));
        }
      }
      
      if (usernames.length === 0) {
        console.error('No valid Twitter usernames/URLs');
        process.exit(1);
      }

      console.log(`Will scrape ${usernames.length} Twitter accounts, up to ${options.count} tweets per account`);
      
      // è®¾ç½®çˆ¬è™«é€‰é¡¹
      const scraperOptions = {
        outputDir,
        tweetCount: options.count,
        separateFiles: options.separate,
        headless: options.headless,
        mergeResults: options.merge,
        mergeFilename: options.mergeFile,
        exportFormat: options.format,
        withReplies,
        exportCsv: !!options.csv,
        exportJson: !!options.json,
        timezone
      };
      
      // æ‰§è¡ŒæŠ“å–ï¼ˆç»Ÿä¸€é€»è¾‘ï¼‰
      const results = await scraper.scrapeTwitterUsers(usernames, scraperOptions);

      console.log(`âœ… Completed! Base output directory: ${outputDir}`);

      // æ˜¾ç¤ºç»“æœæ‘˜è¦
      if (results && results.length > 0) {
        console.log('\nğŸ“Š Scraping results summary:');
        results.forEach(result => {
          const p = result.profile || {};
          const meta = [];
          if (p.displayName) meta.push(`${p.displayName}`);
          if (typeof p.followers === 'number') meta.push(`Followers: ${p.followers}`);
          if (typeof p.following === 'number') meta.push(`Following: ${p.following}`);
          console.log(`- @${result.username}: ${result.tweetCount} tweets${meta.length ? ' | ' + meta.join(' Â· ') : ''}`);
        });

        const runDirs = results
          .map(result => result.runContext?.runDir)
          .filter(Boolean);
        if (runDirs.length > 0) {
          console.log('\nğŸ“‚ Output directories:');
          runDirs.forEach(dir => console.log(`- ${dir}`));
        }
      }
    } catch (error) {
      console.error(`âŒ Error: ${error.message}`);
      if (options.debug) {
        console.error(error);
      }
      process.exit(1);
    }
  });


// è°ƒåº¦å™¨å‘½ä»¤
program
  .command('schedule')
  .description('Run crawler task on schedule')
  .option('-c, --config <filepath>', 'Configuration file path', './crawler-config.json')
  .option('-i, --interval <minutes>', 'Scraping interval (minutes)', '30')
  .option('--headless <boolean>', 'Run browser in headless mode', 'true')
  .option('--timezone <timezone>', 'Timezone for timestamp output (IANA name)')
  .action(async (options) => {
    try {
      // æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
      if (!fs.existsSync(options.config)) {
        console.error(`Error: Config file ${options.config} does not exist`);
        process.exit(1);
      }
      
      options.headless = options.headless === 'true';
      const intervalMinutes = parseInt(options.interval);
      const outputDir = path.resolve(options.parent.output);
      
      // ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
      try {
        await fileUtils.ensureDirExists(outputDir);
      } catch (error) {
        console.error(`Failed to create output directory: ${outputDir}`, error);
        process.exit(1);
      }

      // è°ƒåº¦é€»è¾‘
      console.log(`ğŸ•’ Starting scheduled task, running every ${intervalMinutes} minutes`);
      
      // ç¬¬ä¸€æ¬¡ç«‹å³è¿è¡Œ
      executeScheduledTask();
      
      // è®¾ç½®å®šæ—¶å™¨
      setInterval(executeScheduledTask, intervalMinutes * 60 * 1000);
      
      // è°ƒåº¦æ‰§è¡Œå‡½æ•°
      async function executeScheduledTask() {
        try {
          const now = new Date();
          console.log(`\n[${now.toISOString()}] Executing scheduled scraping task...`);
          
          // åŠ è½½é…ç½®
          const config = JSON.parse(fs.readFileSync(options.config, 'utf8'));

          const timezoneInput =
            (config.schedule && config.schedule.timezone) ||
            options.timezone ||
            timeUtils.getDefaultTimezone();
          const timezone = timeUtils.resolveTimezone(timezoneInput);
          console.log(`Timezone for this run: ${timezone}`);
          
          // åŸºæœ¬é€‰é¡¹
          const scraperOptions = {
            outputDir,
            headless: options.headless,
            mergeResults: options.parent.merge,
            mergeFilename: `${options.parent.mergeFile}-${getFormattedDate()}`,
            exportFormat: options.parent.format,
            timezone
          };
          
          // ä»…æŠ“å–Twitter
          if (config.twitter && (config.twitter.usernames || config.twitter.usernameFile)) {
            let usernames = [];
            if (config.twitter.usernames && Array.isArray(config.twitter.usernames)) {
              usernames = config.twitter.usernames;
            } else if (config.twitter.usernameFile && fs.existsSync(config.twitter.usernameFile)) {
              const fileContent = fs.readFileSync(config.twitter.usernameFile, 'utf8');
              usernames = fileContent.split('\n')
                .map(line => line.trim())
                .filter(line => line && !line.startsWith('#'));
            }
            
            if (usernames.length > 0) {
              const twitterOptions = {
                ...scraperOptions,
                tweetCount: config.twitter.tweetCount || 20,
                separateFiles: config.twitter.separateFiles || false
              };
              
              await scraper.scrapeTwitterUsers(usernames, twitterOptions);
            }
          }
          
          console.log(`âœ… Scheduled task completed!`);
        } catch (schedulerError) {
          console.error(`âŒ Scheduled task error: ${schedulerError.message}`);
          if (options.parent.debug) {
            console.error(schedulerError);
          }
          // ä¸é€€å‡ºè¿›ç¨‹ï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡è°ƒåº¦
        }
      }
      
      // è¾…åŠ©å‡½æ•° - è·å–æ ¼å¼åŒ–æ—¥æœŸ
      function getFormattedDate() {
        const today = new Date();
        return `${today.getFullYear()}-${String(today.getMonth() + 1).padStart(2, '0')}-${String(today.getDate()).padStart(2, '0')}`;
      }
      
      // ä¿æŒè¿›ç¨‹æ´»è·ƒ
      console.log('Scheduler started, press Ctrl+C to exit...');
    } catch (error) {
      console.error(`âŒ Error: ${error.message}`);
      if (options.parent.debug) {
        console.error(error);
      }
      process.exit(1);
    }
  });

// ä¾‹å­å‘½ä»¤
program
  .command('examples')
  .description('Show usage examples')
  .action(() => {
    console.log(`
Twitter/X Crawler Usage Examples:

Scrape a single Twitter account (username):
  $ node cli.js twitter -u elonmusk -c 50 -o ./output

Scrape a single Twitter account (profile URL):
  $ node cli.js twitter -U https://x.com/elonmusk -c 50 -o ./output

Scrape multiple Twitter accounts from file (can mix usernames/@handles/profile URLs):
  $ node cli.js twitter -f twitter_accounts.txt -c 20 -o ./output --merge

Scheduled scraping:
  $ node cli.js schedule -c ./crawler-config.json -i 60 -o ./output

Example config file (crawler-config.json):
{
  "twitter": {
    "usernames": ["elonmusk", "BillGates"],
    "tweetCount": 50,
    "separateFiles": true,
    "useAxios": false
  }
}
`);
  });

// ç›´æ¥è¿è¡Œ
if (require.main === module) {
  program.parse(process.argv);
}

module.exports = program; 
